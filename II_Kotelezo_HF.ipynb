{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "II. Kotelezo HF",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPNGQgO6aXtZ"
      },
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEKuTE7hGsZr"
      },
      "source": [
        "Tesztelő szkript letöltése, importok."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc9fa3F-T0u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e993546-38e3-4ea8-b452-9a3adf7cfe73"
      },
      "source": [
        "# Download tester\n",
        "!rm annbsc22_p1_hw2_tester.py\n",
        "!wget http://nipg12.inf.elte.hu/~vavsaai@nipg.lab/annbsc22_p1_hw2/annbsc22_p1_hw2_tester.py \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.models\n",
        "import tensorflow.keras.optimizers\n",
        "import tensorflow.keras.layers\n",
        "import tensorflow.keras.activations\n",
        "import tensorflow.keras.callbacks\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import tester after importing tensorflow, to make sure correct tf version is imported\n",
        "from annbsc22_p1_hw2_tester import Tester\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-13 20:24:06--  http://nipg12.inf.elte.hu/~vavsaai@nipg.lab/annbsc22_p1_hw2/annbsc22_p1_hw2_tester.py\n",
            "Resolving nipg12.inf.elte.hu (nipg12.inf.elte.hu)... 157.181.160.172\n",
            "Connecting to nipg12.inf.elte.hu (nipg12.inf.elte.hu)|157.181.160.172|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42851 (42K) [text/plain]\n",
            "Saving to: ‘annbsc22_p1_hw2_tester.py’\n",
            "\n",
            "annbsc22_p1_hw2_tes 100%[===================>]  41.85K   168KB/s    in 0.2s    \n",
            "\n",
            "2022-05-13 20:24:07 (168 KB/s) - ‘annbsc22_p1_hw2_tester.py’ saved [42851/42851]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ7TTytUaUwW"
      },
      "source": [
        "###**A**: Az adatbázis letöltése\n",
        "\n",
        "**A/1 - A/6**: A tesztelőszkript a számodra szükséges adatbázist tölti le és a részedre kiosztott feladatok helyességét fogja tesztelni. Ehhez az alábbi kódblokk tetején, a Tester példányosításakor paraméterként a Neptun-kódodat kell megadni egy sztringként, pl: \n",
        "`tester = Tester('DK7UAQ')`\n",
        "\n",
        "**Információ az adatbázisokról**:\n",
        "Az adatbázisok a UCI Machine Learning Repository-ról származnak.\n",
        "- **A/1**: Különböző vegyületekre (a vegyületek tulajdonságai alapján) próbáljuk megbecsülni azt a mennyiséget, ami az esetek 50%-ában megöli a _Pimephales promelas_ fajtájú halat. http://archive.ics.uci.edu/ml/datasets/QSAR+fish+toxicity\n",
        "- **A/2**: Két portugál középiskolában a tanulók évvégi matematika jegyét (0-20) próbáljuk megbecsülni a tanulók szociális és pénzügyi háttere alapján. http://archive.ics.uci.edu/ml/datasets/Student+Performance \n",
        "- **A/3**: Lásd A/2, matematika helyett portugál nyelv tárgyból.\n",
        "- **A/4**: Lásd A/2. Az A/2. és A/3.-ban használt adatbázisok esetében az input változók közt megtalálhatók a tanulók első- és második harmadévvégi osztályzata az adott tárgyból. Ezekkel nyilvánvalóan erősen korrelál az év végi jegy, így nehezíti a feladatot, ha ezeket az input változókat elhagyjuk. Az A/4 és A/5. adatbázisaiból ezek a változók hiányoznak, így jelentősen gyengébb eredmény várható.\n",
        "- **A/5**: Lásd A/4, matematika helyett portugál nyelv tárgyból.\n",
        "- **A/6**: Betontömbök nyomószilárdságát (~1-100 MPa) próbáljuk megbecsülni a betontömbök összetevőinek mennyiségéből, előállítási körülményeikből és korukból. http://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength\n",
        "\n",
        "A tesztelőszkript letölti a szöveges formában tárolt adatbázist és elhelyezi a `content` sztringben. Alább, kiírjuk a hosszát és az első 500 karaktert is a sztringből. Az adatbázis fájlok egy-egy sora adja meg az egy-egy mintaelemhez tartozó adatokat, ahol az utolsó érték a címke, míg a többi érték az input változók értékeit kódolja. Az adatbázis első sora a változók neveit tartalmazza.\n",
        "\n",
        "A feladatod, hogy alakítsd át a `content` sztringet az adatbázis input változóit és címkéit tartalmazó tömbbé. A tömbök típusa np.float32 (lebegőpontos) típusú legyen! Az input változók értékeit helyezd el a `features` (n_mintaelem, n_valtozo) alakú tömbben, míg a címkéket a `labels` (n_mintaelem,) alakú tömbben!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdKqP5OvXJpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c823588d-29e7-4471-f5e8-0c935a41b221"
      },
      "source": [
        "tester = Tester('DITP5V')\n",
        "content = tester.get_dataset_content()\n",
        "\n",
        "print(len(content))\n",
        "print(content[:500])\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59285\n",
            "Cement (component 1)(kg in a m^3 mixture),Blast Furnace Slag (component 2)(kg in a m^3 mixture),Fly Ash (component 3)(kg in a m^3 mixture),Water  (component 4)(kg in a m^3 mixture),Superplasticizer (component 5)(kg in a m^3 mixture),Coarse Aggregate  (component 6)(kg in a m^3 mixture),Fine Aggregate (component 7)(kg in a m^3 mixture),Age (day),\"Concrete compressive strength(MPa, megapascals) \"\r\n",
            "540.0 ,0.0 ,0.0 ,162.0 ,2.5 ,1040.0 ,676.0 ,28 ,79.99 \r\n",
            "540.0 ,0.0 ,0.0 ,162.0 ,2.5 ,1055.0 ,676.0 ,28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_kfEC4sEMtw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bad1c62-d2f8-4688-e6e4-3d40dbaaa5e0"
      },
      "source": [
        "# implement your solution BELOW\n",
        "import re\n",
        "\n",
        "def preprocess_dataset(data_text):\n",
        "  lines = data_text.split(\"\\n\")\n",
        "  words = [re.split(r',(?=[^\\s]+)', line) for line in lines]\n",
        "  attr_names = words[0]\n",
        "  vals = words[1:]\n",
        "  vals = [[item.replace(' ', '') for item in rec] for rec in vals]\n",
        "  vals = [[item.replace('\\r', '') for item in rec] for rec in vals]\n",
        "  vals = [rec for rec in vals if rec != ['']]   # the last element is a list of an empty string\n",
        "  vals = [[float(item) for item in rec] for rec in vals]\n",
        "  vals = np.array(vals, dtype=np.float32)\n",
        "  labels = vals[:, -1]\n",
        "  vals = vals[:, :-1]\n",
        "  return attr_names, vals, labels\n",
        "\n",
        "attr_names, features, labels = preprocess_dataset(content)\n",
        "\n",
        "# implement your solution ABOVE\n",
        "tester.test('dataset_shape', features, labels)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tester: Dataset preparation OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrE7NLJTR7Aq"
      },
      "source": [
        "## **B**: Tanító-, validációs- és teszthalmaz szétválasztása\n",
        "\n",
        "Keverd meg véletlenszerűen a mintaelemeket a `features` és `labels` tömbökben (természetesen a két tömb permutálása legyen azonos)! Ez azért szükséges, mert az adatbázisban az elemek lehet, hogy rendezettek pl. a címkék szerint. Megkeverés nélkül, a szétválasztott halmazokban jelentősen különbözne a címkék eloszlása. \n",
        "\n",
        "Ezt követően válaszd szét a tömböket tanító-, validációs- és teszthalmazra!\n",
        "\n",
        "A szétválasztás történjen az alábbi arányokkal:\n",
        "- **B/1**.: 70% / 15% / 15%\n",
        "- **B/2**: 60% / 20% / 20%\n",
        "- **B/3**: 50% / 25% / 25%\n",
        "\n",
        "A halmazokat tartalmazó három-három input- és címketömb változókat nevezd el `x_unnorm_train`, `x_unnorm_val`, `x_unnorm_test`, `y_train`, `y_val`, `y_test`-nek sorrendben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls4niplvUgQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19efc44a-bc77-493c-c31c-63c7653eaf23"
      },
      "source": [
        "# implement your solution BELOW\n",
        "import random\n",
        "\n",
        "array_to_shuffle = np.c_[features, labels]\n",
        "np.random.shuffle(array_to_shuffle)\n",
        "\n",
        "features, labels = array_to_shuffle[:, :-1], array_to_shuffle[:, -1]\n",
        "\n",
        "x_unnorm_train = features[:720]\n",
        "y_train = labels[:720]\n",
        "\n",
        "x_unnorm_val = features[720:875]\n",
        "y_val = labels[720:875]\n",
        "\n",
        "x_unnorm_test = features[875:]\n",
        "y_test = labels[875:]\n",
        "\n",
        "# implement your solution ABOVE\n",
        "tester.test('dataset_split', x_unnorm_train, x_unnorm_val, x_unnorm_test,\\\n",
        "                             y_train, y_val, y_test)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tester: Dataset split OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjm2zVN92yGq"
      },
      "source": [
        "## **C**: Az input változók azonos nagyságrendre hozása\n",
        "\n",
        "A változók azonos nagyságrendre hozása az alábbi technikával történjen:\n",
        "- **C/1**: sztenderdizáció (0 átlag, 1 szórás)\n",
        "- **C/2**: min-max skálázás (0 minimum, 1 maximum)\n",
        "\n",
        "A változókat egymástól függetlenül kell skálázni. A skálázáshoz szükséges átlagot és szórást / minimumot és maximumot a tanítóhalmazon számold ki, majd alkalmazd őket a validációs és a teszt halmazra. Ez azért szükséges, mert a teszthalmazról azt feltételezzük, hogy ismeretlen a betanítás során, pontosan azért, hogy semmi esetre se tudjuk hozzáigazítani a tanított modellünket. \n",
        "\n",
        "**A feladatot *NumPy* segítségével kell megoldani. Ennek a részfeladatnak a megoldásához nem megengedett a *Keras* és a *TensorFlow* könyvtárnak és azok almoduljainak használata.**\n",
        "\n",
        "A skálázott halmazokat tartalmazó három változót nevezd el `x_train`, `x_val`, `x_test`-nek sorrendben!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9SLPxkO3RZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71112f6-df53-4573-971f-f9ed596a78b0"
      },
      "source": [
        "# implement your solution BELOW\n",
        "x_unnorm_train_min = np.amin(x_unnorm_train, axis=0)\n",
        "x_unnorm_train_max = np.amax(x_unnorm_train, axis=0)\n",
        "\n",
        "x_train = (x_unnorm_train - x_unnorm_train_min) / (x_unnorm_train_max - x_unnorm_train_min)\n",
        "x_val = (x_unnorm_val - x_unnorm_train_min) / (x_unnorm_train_max - x_unnorm_train_min)\n",
        "x_test = (x_unnorm_test - x_unnorm_train_min) / (x_unnorm_train_max - x_unnorm_train_min)\n",
        "\n",
        "# implement your solution ABOVE\n",
        "tester.test('dataset_rescale', x_train, x_val, x_test)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tester: Feature rescale OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dohOMs4NlVXG"
      },
      "source": [
        "## **D**: A regressziós neuronháló definiálása Keras-ban \n",
        "\n",
        "Használd a `keras.models.Sequential()` osztályt a háló definiálásához! A háló architektúráját az alábbiak alapján alakítsd ki:\n",
        "- **D/1**: Kétrétegű háló, az első réteg neuronjainak száma 50, aktivációs függvénye ReLU. Az első réteg után alkalmazz dropout-ot neurononként 30%-os valószínűséggel!\n",
        "- **D/2**: Négyrétegű háló, a neuronok száma rendre 20, 20, 10, aktivációs függvényük ReLU. Az egyes rétegek közé tégy dropout rétegeket, melyek 20%-os valószínűséggel nulláznak ki egy-egy elemet!\n",
        "- **D/3**: Háromrétegű háló, a neuronok száma rendre 50, 30. Aktivációs függvényként használj sorban tanh-t, majd ReLU-t!\n",
        "\n",
        "Minden esetben csak teljesen összekötött (dense) rétegeket, valamint dropout rétegeket kell használnod. Az aktivációs függvényeket a teljesen összekötött rétegek `activation` paramétereként add meg (azaz most ne használj külön Acitvation rétegeket). A rétegek számába beleértendő a kimenetre képző, utolsó réteg is, melynek méretét a címke mérete határozza meg. Alkalmazd a tanult aktivációs függvényt az utolsó rétegen, függően a feladat típusától.\n",
        "A modellt fordítsd le a `compile()` függvény segítségével: a költségfüggvény legyen a szokásos, regresszióhoz használatos átlagos négyzetes eltérés (MSE), optimizer algoritmusként pedig alkalmazd az SGD algoritmust megfelelő tanulási rátával! \n",
        "A regressziós neuronhálót tartalmazó keras.models.Sequential() típusú változót nevezd el `reg_model`-nek!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAywpNOml4VL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa5abd9-ae2a-44b8-a62f-fe25bb27211c"
      },
      "source": [
        "# implement your solution BELOW\n",
        "tensorflow.random.set_seed(42)\n",
        "\n",
        "reg_model = tensorflow.keras.models.Sequential()\n",
        "reg_model.add(tensorflow.keras.layers.Dense(50, activation=\"relu\", input_dim=x_train.shape[1]))\n",
        "reg_model.add(tensorflow.keras.layers.Dropout(0.3))\n",
        "reg_model.add(tensorflow.keras.layers.Dense(1))\n",
        "\n",
        "reg_model.compile(optimizer=tensorflow.keras.optimizers.SGD(learning_rate=0.01), loss=\"mse\", metrics=['mean_absolute_error'])\n",
        "\n",
        "# implement your solution ABOVE\n",
        "\n",
        "tester.test('reg_model_architecture', reg_model)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tester: Regression model architecture OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz4fFtBUhUXV"
      },
      "source": [
        "## **E**: A háló betanítása regressziós feladatra\n",
        "\n",
        "Tanítsd be a neuronhálót a tanítóhalmazon! Használj early stopping-ot a validációs halmazzal! A tanulási rátát, epoch-ok maximális számát, a batch méretet, az early stopping `patience` paraméterét szabadon átállíthatod. Próbálgathatsz különböző kombinációkat, hogy jobb eredményt érj el.\n",
        "\n",
        "- Rajzold ki, hogyan alakult a tanulási és validációs költség a betanítás során! Ehhez használhatod a `matplotlib` könyvtárat, példát találsz a 8. előadás notebookjában. \n",
        "- A négyzetes (MSE) költség mellett az átlagos abszolút hiba (Mean Absolute Error, MAE) tanító- és validációs halmazon történő alakulását is rajzold ki egy másik grafikonon! A grafikonok y tengelyének megjelenített értékhatárait úgy állítsd be, hogy mindegyik idősor látható legyen és könnyen ki lehessen venni a költségek alakulását a tanítás vége felé is!\n",
        "- Számold ki a betanított modell négyzetes hibáját és az átlagos abszolút hibát a teszthalamzon, majd add értékül ezeket az értékeket a `test_mse` és `test_mae` változóknak!\n",
        "- Válassz néhány példát a teszthalmazból és becsülj hozzájuk címkét a betanított modellel, majd írasd ki a becsült és a megfelelő helyes címkéket! Így példákon is láthatjuk, hogy mennyire ad jó becsléseket a neuronhálónk.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQf90jJPkeUl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1dfb372-2474-42bd-d41c-d2bd397d667e"
      },
      "source": [
        "# implement your solution BELOW\n",
        "\n",
        "earlystopping_callback = tensorflow.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=60)\n",
        "\n",
        "history = reg_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=64, epochs=400, verbose=1, callbacks=[earlystopping_callback])\n",
        "\n",
        "tr_losses = history.history['loss']\n",
        "val_losses = history.history['val_loss']\n",
        "tr_mae = history.history['mean_absolute_error']\n",
        "val_mae = history.history['val_mean_absolute_error']\n",
        "\n",
        "test_mse, test_mae = reg_model.evaluate(x_test, y_test)\n",
        "\n",
        "#y_test_predicted = reg_model.predict(x_test)\n",
        "#test_mae = np.mean(np.abs(y_test_predicted - y_test.reshape(155,1)))\n",
        "\n",
        "print(\"Final training loss: \", tr_losses[-1])\n",
        "print(\"Final validation loss: \", val_losses[-1])\n",
        "print(\"Final training mae loss: \", tr_mae[-1])\n",
        "print(\"Final validation mae loss: \", val_mae[-1])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "fig.suptitle('MSE and MAE losses')\n",
        "\n",
        "ax1.plot(tr_losses, label=\"J_train\")\n",
        "ax1.plot(val_losses, label=\"J_val\")\n",
        "ax1.set(xlabel=\"Number of epochs\", ylabel=\"Cost (J)\")\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(tr_mae, label=\"M_train\")\n",
        "ax2.plot(val_mae, label=\"M_val\")\n",
        "ax2.set(xlabel=\"Number of epochs\", ylabel=\"MAE (M)\")\n",
        "ax2.legend()\n",
        "\n",
        "x_test_part = x_test[:5]\n",
        "y_test_part = y_test[:5]\n",
        "\n",
        "predictions = reg_model.predict(x_test_part)\n",
        "print(\"Predictions: \", predictions)\n",
        "print(\"True labels: \", y_test_part)\n",
        "\n",
        "# implement your solution ABOVE\n",
        "\n",
        "tester.test('reg_model_learning', test_mse, test_mae)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "12/12 [==============================] - 1s 17ms/step - loss: 624.5578 - mean_absolute_error: 19.6885 - val_loss: 192.3698 - val_mean_absolute_error: 11.2385\n",
            "Epoch 2/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 307.6320 - mean_absolute_error: 14.1175 - val_loss: 165.3341 - val_mean_absolute_error: 10.5260\n",
            "Epoch 3/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 180.3836 - mean_absolute_error: 10.8820 - val_loss: 326.5922 - val_mean_absolute_error: 14.6936\n",
            "Epoch 4/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 266.8246 - mean_absolute_error: 13.0341 - val_loss: 207.9008 - val_mean_absolute_error: 11.9206\n",
            "Epoch 5/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 248.7648 - mean_absolute_error: 12.6437 - val_loss: 230.8663 - val_mean_absolute_error: 12.5417\n",
            "Epoch 6/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 282.0305 - mean_absolute_error: 13.3768 - val_loss: 291.2010 - val_mean_absolute_error: 14.2563\n",
            "Epoch 7/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 186.2410 - mean_absolute_error: 10.5151 - val_loss: 145.4676 - val_mean_absolute_error: 9.8912\n",
            "Epoch 8/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 148.5924 - mean_absolute_error: 9.7230 - val_loss: 155.5061 - val_mean_absolute_error: 10.1837\n",
            "Epoch 9/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 147.8962 - mean_absolute_error: 9.5819 - val_loss: 128.8651 - val_mean_absolute_error: 9.1177\n",
            "Epoch 10/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 157.6558 - mean_absolute_error: 10.1005 - val_loss: 199.7185 - val_mean_absolute_error: 11.5169\n",
            "Epoch 11/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 150.2687 - mean_absolute_error: 9.7995 - val_loss: 126.1216 - val_mean_absolute_error: 8.9182\n",
            "Epoch 12/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 137.3120 - mean_absolute_error: 9.1756 - val_loss: 132.8111 - val_mean_absolute_error: 9.3017\n",
            "Epoch 13/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 170.0405 - mean_absolute_error: 10.2218 - val_loss: 123.1435 - val_mean_absolute_error: 8.8459\n",
            "Epoch 14/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 183.8148 - mean_absolute_error: 10.8230 - val_loss: 123.9896 - val_mean_absolute_error: 8.9159\n",
            "Epoch 15/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 150.3095 - mean_absolute_error: 9.6220 - val_loss: 126.3530 - val_mean_absolute_error: 8.9141\n",
            "Epoch 16/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 137.9332 - mean_absolute_error: 9.1902 - val_loss: 126.0012 - val_mean_absolute_error: 9.0179\n",
            "Epoch 17/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 139.9812 - mean_absolute_error: 9.3940 - val_loss: 135.9199 - val_mean_absolute_error: 9.5082\n",
            "Epoch 18/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 132.7348 - mean_absolute_error: 9.2502 - val_loss: 118.9193 - val_mean_absolute_error: 8.6557\n",
            "Epoch 19/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 131.2944 - mean_absolute_error: 9.1719 - val_loss: 146.5460 - val_mean_absolute_error: 9.9363\n",
            "Epoch 20/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 172.5765 - mean_absolute_error: 10.4780 - val_loss: 160.9156 - val_mean_absolute_error: 10.3628\n",
            "Epoch 21/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 141.2346 - mean_absolute_error: 9.3762 - val_loss: 281.5746 - val_mean_absolute_error: 13.9935\n",
            "Epoch 22/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 138.5086 - mean_absolute_error: 9.5482 - val_loss: 119.6237 - val_mean_absolute_error: 8.6987\n",
            "Epoch 23/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 119.7551 - mean_absolute_error: 8.8186 - val_loss: 257.5963 - val_mean_absolute_error: 12.9404\n",
            "Epoch 24/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 165.6294 - mean_absolute_error: 10.0375 - val_loss: 188.3510 - val_mean_absolute_error: 10.7394\n",
            "Epoch 25/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 159.9454 - mean_absolute_error: 9.9197 - val_loss: 260.7528 - val_mean_absolute_error: 13.3287\n",
            "Epoch 26/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 150.3748 - mean_absolute_error: 9.6781 - val_loss: 198.5533 - val_mean_absolute_error: 11.5468\n",
            "Epoch 27/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 135.5712 - mean_absolute_error: 9.1507 - val_loss: 120.3271 - val_mean_absolute_error: 8.8639\n",
            "Epoch 28/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 126.7379 - mean_absolute_error: 8.9205 - val_loss: 137.5763 - val_mean_absolute_error: 9.0761\n",
            "Epoch 29/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 133.3195 - mean_absolute_error: 9.3045 - val_loss: 185.5148 - val_mean_absolute_error: 10.5233\n",
            "Epoch 30/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 145.8221 - mean_absolute_error: 9.4290 - val_loss: 119.6283 - val_mean_absolute_error: 8.7019\n",
            "Epoch 31/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 146.7227 - mean_absolute_error: 9.5167 - val_loss: 194.2142 - val_mean_absolute_error: 10.7497\n",
            "Epoch 32/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 133.9579 - mean_absolute_error: 9.1235 - val_loss: 120.1600 - val_mean_absolute_error: 8.7756\n",
            "Epoch 33/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 130.8851 - mean_absolute_error: 9.0957 - val_loss: 284.8829 - val_mean_absolute_error: 14.0068\n",
            "Epoch 34/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 139.3219 - mean_absolute_error: 9.2597 - val_loss: 144.2106 - val_mean_absolute_error: 9.7752\n",
            "Epoch 35/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 141.5157 - mean_absolute_error: 9.4967 - val_loss: 136.1761 - val_mean_absolute_error: 9.4967\n",
            "Epoch 36/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 132.9232 - mean_absolute_error: 9.2163 - val_loss: 205.5418 - val_mean_absolute_error: 11.7407\n",
            "Epoch 37/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 150.1768 - mean_absolute_error: 9.6323 - val_loss: 351.4868 - val_mean_absolute_error: 15.4769\n",
            "Epoch 38/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 155.2641 - mean_absolute_error: 9.7168 - val_loss: 132.9195 - val_mean_absolute_error: 9.0481\n",
            "Epoch 39/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 145.5353 - mean_absolute_error: 9.5865 - val_loss: 115.0028 - val_mean_absolute_error: 8.5589\n",
            "Epoch 40/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 126.5650 - mean_absolute_error: 8.8609 - val_loss: 163.2892 - val_mean_absolute_error: 10.4005\n",
            "Epoch 41/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 142.3234 - mean_absolute_error: 9.5002 - val_loss: 137.0700 - val_mean_absolute_error: 9.5582\n",
            "Epoch 42/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 128.5490 - mean_absolute_error: 9.0709 - val_loss: 119.7145 - val_mean_absolute_error: 8.8746\n",
            "Epoch 43/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 122.7838 - mean_absolute_error: 8.8841 - val_loss: 121.0715 - val_mean_absolute_error: 8.5764\n",
            "Epoch 44/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 141.7021 - mean_absolute_error: 9.3437 - val_loss: 127.7696 - val_mean_absolute_error: 9.1593\n",
            "Epoch 45/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 131.7563 - mean_absolute_error: 9.0281 - val_loss: 152.1260 - val_mean_absolute_error: 9.5601\n",
            "Epoch 46/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 130.2975 - mean_absolute_error: 8.8080 - val_loss: 123.1052 - val_mean_absolute_error: 8.6893\n",
            "Epoch 47/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 126.8538 - mean_absolute_error: 8.7645 - val_loss: 121.6234 - val_mean_absolute_error: 8.9080\n",
            "Epoch 48/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 121.9237 - mean_absolute_error: 8.7237 - val_loss: 114.6946 - val_mean_absolute_error: 8.4306\n",
            "Epoch 49/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 128.1612 - mean_absolute_error: 8.9752 - val_loss: 120.4619 - val_mean_absolute_error: 8.5542\n",
            "Epoch 50/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 139.6889 - mean_absolute_error: 9.2404 - val_loss: 123.0339 - val_mean_absolute_error: 8.6751\n",
            "Epoch 51/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 140.5980 - mean_absolute_error: 9.3706 - val_loss: 113.9085 - val_mean_absolute_error: 8.4611\n",
            "Epoch 52/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 120.5250 - mean_absolute_error: 8.7017 - val_loss: 177.4916 - val_mean_absolute_error: 10.4056\n",
            "Epoch 53/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 134.7552 - mean_absolute_error: 9.1969 - val_loss: 115.5690 - val_mean_absolute_error: 8.6595\n",
            "Epoch 54/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 116.9327 - mean_absolute_error: 8.5895 - val_loss: 108.4320 - val_mean_absolute_error: 8.3915\n",
            "Epoch 55/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 131.5826 - mean_absolute_error: 9.0555 - val_loss: 110.8692 - val_mean_absolute_error: 8.3730\n",
            "Epoch 56/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 131.1978 - mean_absolute_error: 8.9389 - val_loss: 207.3262 - val_mean_absolute_error: 11.7941\n",
            "Epoch 57/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 128.3752 - mean_absolute_error: 8.8730 - val_loss: 119.7967 - val_mean_absolute_error: 8.8575\n",
            "Epoch 58/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 131.0432 - mean_absolute_error: 9.1060 - val_loss: 104.5054 - val_mean_absolute_error: 8.2506\n",
            "Epoch 59/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 107.5016 - mean_absolute_error: 8.2357 - val_loss: 103.7980 - val_mean_absolute_error: 8.0830\n",
            "Epoch 60/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 116.7784 - mean_absolute_error: 8.5718 - val_loss: 145.1944 - val_mean_absolute_error: 9.7514\n",
            "Epoch 61/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 125.4586 - mean_absolute_error: 8.7092 - val_loss: 113.7031 - val_mean_absolute_error: 8.3340\n",
            "Epoch 62/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 121.0896 - mean_absolute_error: 8.6135 - val_loss: 114.8742 - val_mean_absolute_error: 8.6145\n",
            "Epoch 63/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 121.6062 - mean_absolute_error: 8.6303 - val_loss: 102.6617 - val_mean_absolute_error: 8.0715\n",
            "Epoch 64/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 120.7607 - mean_absolute_error: 8.7651 - val_loss: 144.8699 - val_mean_absolute_error: 9.4307\n",
            "Epoch 65/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 128.8228 - mean_absolute_error: 9.0128 - val_loss: 170.0298 - val_mean_absolute_error: 10.4762\n",
            "Epoch 66/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 134.0673 - mean_absolute_error: 9.1472 - val_loss: 103.4701 - val_mean_absolute_error: 8.1921\n",
            "Epoch 67/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 118.8327 - mean_absolute_error: 8.5785 - val_loss: 99.0848 - val_mean_absolute_error: 7.9315\n",
            "Epoch 68/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 119.9448 - mean_absolute_error: 8.6332 - val_loss: 175.4147 - val_mean_absolute_error: 10.7434\n",
            "Epoch 69/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 125.2432 - mean_absolute_error: 8.7476 - val_loss: 125.4270 - val_mean_absolute_error: 9.0036\n",
            "Epoch 70/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 99.3848 - mean_absolute_error: 7.8320 - val_loss: 104.6015 - val_mean_absolute_error: 8.1509\n",
            "Epoch 71/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 104.6700 - mean_absolute_error: 7.9075 - val_loss: 163.2957 - val_mean_absolute_error: 10.1948\n",
            "Epoch 72/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 125.5298 - mean_absolute_error: 8.8693 - val_loss: 105.0453 - val_mean_absolute_error: 7.9525\n",
            "Epoch 73/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 110.9714 - mean_absolute_error: 8.1277 - val_loss: 207.8641 - val_mean_absolute_error: 11.8223\n",
            "Epoch 74/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 121.5720 - mean_absolute_error: 8.7087 - val_loss: 97.8423 - val_mean_absolute_error: 7.9277\n",
            "Epoch 75/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 116.5977 - mean_absolute_error: 8.3671 - val_loss: 129.5437 - val_mean_absolute_error: 9.1715\n",
            "Epoch 76/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 117.4818 - mean_absolute_error: 8.5501 - val_loss: 156.5238 - val_mean_absolute_error: 10.0209\n",
            "Epoch 77/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 122.2568 - mean_absolute_error: 8.4959 - val_loss: 86.7872 - val_mean_absolute_error: 7.3475\n",
            "Epoch 78/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 94.0491 - mean_absolute_error: 7.5307 - val_loss: 92.9780 - val_mean_absolute_error: 7.7442\n",
            "Epoch 79/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 126.6586 - mean_absolute_error: 8.7132 - val_loss: 172.3360 - val_mean_absolute_error: 10.5743\n",
            "Epoch 80/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 106.0477 - mean_absolute_error: 7.9144 - val_loss: 87.2868 - val_mean_absolute_error: 7.1863\n",
            "Epoch 81/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 107.0986 - mean_absolute_error: 8.0517 - val_loss: 85.7104 - val_mean_absolute_error: 7.3969\n",
            "Epoch 82/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 99.4977 - mean_absolute_error: 7.7649 - val_loss: 86.1116 - val_mean_absolute_error: 7.4730\n",
            "Epoch 83/400\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 114.2796 - mean_absolute_error: 8.3998 - val_loss: 138.6838 - val_mean_absolute_error: 9.4208\n",
            "Epoch 84/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 106.0538 - mean_absolute_error: 8.0728 - val_loss: 87.6688 - val_mean_absolute_error: 7.4315\n",
            "Epoch 85/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 105.3655 - mean_absolute_error: 8.0304 - val_loss: 93.0127 - val_mean_absolute_error: 7.6754\n",
            "Epoch 86/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 113.7720 - mean_absolute_error: 8.2839 - val_loss: 89.7613 - val_mean_absolute_error: 7.4774\n",
            "Epoch 87/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 103.8743 - mean_absolute_error: 7.9946 - val_loss: 82.9461 - val_mean_absolute_error: 7.1497\n",
            "Epoch 88/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 110.6463 - mean_absolute_error: 8.1662 - val_loss: 80.6697 - val_mean_absolute_error: 6.9873\n",
            "Epoch 89/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 98.1731 - mean_absolute_error: 7.6922 - val_loss: 79.6219 - val_mean_absolute_error: 6.8857\n",
            "Epoch 90/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 117.8505 - mean_absolute_error: 8.5006 - val_loss: 92.4108 - val_mean_absolute_error: 7.7447\n",
            "Epoch 91/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 94.5438 - mean_absolute_error: 7.4661 - val_loss: 81.3503 - val_mean_absolute_error: 6.8914\n",
            "Epoch 92/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 105.0611 - mean_absolute_error: 7.8160 - val_loss: 83.3135 - val_mean_absolute_error: 7.1054\n",
            "Epoch 93/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 108.9633 - mean_absolute_error: 8.0944 - val_loss: 98.7136 - val_mean_absolute_error: 7.6018\n",
            "Epoch 94/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 96.9217 - mean_absolute_error: 7.5963 - val_loss: 117.5620 - val_mean_absolute_error: 8.5313\n",
            "Epoch 95/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 112.7185 - mean_absolute_error: 8.3383 - val_loss: 136.2463 - val_mean_absolute_error: 9.3871\n",
            "Epoch 96/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 111.6466 - mean_absolute_error: 8.0377 - val_loss: 94.8014 - val_mean_absolute_error: 7.7662\n",
            "Epoch 97/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 94.9875 - mean_absolute_error: 7.4011 - val_loss: 80.3945 - val_mean_absolute_error: 7.1554\n",
            "Epoch 98/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 100.2988 - mean_absolute_error: 7.7306 - val_loss: 101.1529 - val_mean_absolute_error: 7.8395\n",
            "Epoch 99/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 115.0164 - mean_absolute_error: 8.2690 - val_loss: 216.1744 - val_mean_absolute_error: 12.3062\n",
            "Epoch 100/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 111.6414 - mean_absolute_error: 8.1462 - val_loss: 109.4081 - val_mean_absolute_error: 8.3148\n",
            "Epoch 101/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 115.7833 - mean_absolute_error: 8.3881 - val_loss: 95.0988 - val_mean_absolute_error: 7.5877\n",
            "Epoch 102/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 108.9860 - mean_absolute_error: 8.2023 - val_loss: 95.9891 - val_mean_absolute_error: 7.5288\n",
            "Epoch 103/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 92.1819 - mean_absolute_error: 7.4111 - val_loss: 76.3412 - val_mean_absolute_error: 6.9551\n",
            "Epoch 104/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 123.0279 - mean_absolute_error: 8.5192 - val_loss: 79.1211 - val_mean_absolute_error: 7.0394\n",
            "Epoch 105/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 102.6585 - mean_absolute_error: 7.8847 - val_loss: 79.2740 - val_mean_absolute_error: 6.8671\n",
            "Epoch 106/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 108.7197 - mean_absolute_error: 7.9978 - val_loss: 95.6720 - val_mean_absolute_error: 7.9322\n",
            "Epoch 107/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 101.4739 - mean_absolute_error: 7.7651 - val_loss: 330.6868 - val_mean_absolute_error: 15.4528\n",
            "Epoch 108/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 126.2879 - mean_absolute_error: 8.7260 - val_loss: 84.4476 - val_mean_absolute_error: 7.4254\n",
            "Epoch 109/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 91.9212 - mean_absolute_error: 7.4602 - val_loss: 90.0821 - val_mean_absolute_error: 7.4975\n",
            "Epoch 110/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 98.5119 - mean_absolute_error: 7.5310 - val_loss: 210.2936 - val_mean_absolute_error: 11.8550\n",
            "Epoch 111/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 117.3734 - mean_absolute_error: 8.3054 - val_loss: 158.8594 - val_mean_absolute_error: 10.0605\n",
            "Epoch 112/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 115.7954 - mean_absolute_error: 8.3412 - val_loss: 77.2133 - val_mean_absolute_error: 7.0044\n",
            "Epoch 113/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 83.6410 - mean_absolute_error: 7.1201 - val_loss: 127.6568 - val_mean_absolute_error: 8.9796\n",
            "Epoch 114/400\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 106.2036 - mean_absolute_error: 8.0017 - val_loss: 87.2011 - val_mean_absolute_error: 7.3158\n",
            "Epoch 115/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 91.4409 - mean_absolute_error: 7.2196 - val_loss: 70.5714 - val_mean_absolute_error: 6.6164\n",
            "Epoch 116/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 97.1727 - mean_absolute_error: 7.4601 - val_loss: 70.4058 - val_mean_absolute_error: 6.3885\n",
            "Epoch 117/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 104.4309 - mean_absolute_error: 7.9176 - val_loss: 79.0125 - val_mean_absolute_error: 7.0299\n",
            "Epoch 118/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 93.2739 - mean_absolute_error: 7.2598 - val_loss: 73.5440 - val_mean_absolute_error: 6.7031\n",
            "Epoch 119/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 90.1520 - mean_absolute_error: 7.1858 - val_loss: 232.8813 - val_mean_absolute_error: 12.8118\n",
            "Epoch 120/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 105.9363 - mean_absolute_error: 7.9415 - val_loss: 78.0272 - val_mean_absolute_error: 6.6688\n",
            "Epoch 121/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 89.1144 - mean_absolute_error: 7.0783 - val_loss: 77.9542 - val_mean_absolute_error: 6.8698\n",
            "Epoch 122/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 111.4681 - mean_absolute_error: 8.1780 - val_loss: 76.2327 - val_mean_absolute_error: 6.7981\n",
            "Epoch 123/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 93.2612 - mean_absolute_error: 7.3352 - val_loss: 133.9498 - val_mean_absolute_error: 9.1347\n",
            "Epoch 124/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 129.4614 - mean_absolute_error: 8.7388 - val_loss: 73.2692 - val_mean_absolute_error: 6.7238\n",
            "Epoch 125/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 102.0601 - mean_absolute_error: 7.8985 - val_loss: 75.4001 - val_mean_absolute_error: 6.8843\n",
            "Epoch 126/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 91.8733 - mean_absolute_error: 7.4101 - val_loss: 66.9326 - val_mean_absolute_error: 6.4050\n",
            "Epoch 127/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 83.6956 - mean_absolute_error: 7.0998 - val_loss: 69.5177 - val_mean_absolute_error: 6.2193\n",
            "Epoch 128/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 112.4861 - mean_absolute_error: 8.1387 - val_loss: 101.7955 - val_mean_absolute_error: 8.0147\n",
            "Epoch 129/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 94.0924 - mean_absolute_error: 7.4689 - val_loss: 127.7182 - val_mean_absolute_error: 9.0366\n",
            "Epoch 130/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 101.0287 - mean_absolute_error: 7.8535 - val_loss: 111.5399 - val_mean_absolute_error: 8.3905\n",
            "Epoch 131/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 111.2502 - mean_absolute_error: 8.2445 - val_loss: 89.3802 - val_mean_absolute_error: 7.2280\n",
            "Epoch 132/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 85.8159 - mean_absolute_error: 7.0067 - val_loss: 110.6680 - val_mean_absolute_error: 8.2175\n",
            "Epoch 133/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 89.6125 - mean_absolute_error: 7.2619 - val_loss: 76.7983 - val_mean_absolute_error: 6.9035\n",
            "Epoch 134/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 97.4646 - mean_absolute_error: 7.5495 - val_loss: 100.1371 - val_mean_absolute_error: 7.8452\n",
            "Epoch 135/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 82.2239 - mean_absolute_error: 7.0879 - val_loss: 97.1887 - val_mean_absolute_error: 7.5224\n",
            "Epoch 136/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 98.6835 - mean_absolute_error: 7.6914 - val_loss: 69.3228 - val_mean_absolute_error: 6.3787\n",
            "Epoch 137/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 93.9217 - mean_absolute_error: 7.4906 - val_loss: 69.4362 - val_mean_absolute_error: 6.5450\n",
            "Epoch 138/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 96.4192 - mean_absolute_error: 7.4668 - val_loss: 85.4699 - val_mean_absolute_error: 7.2453\n",
            "Epoch 139/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 85.5671 - mean_absolute_error: 7.0261 - val_loss: 63.7239 - val_mean_absolute_error: 6.0609\n",
            "Epoch 140/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 90.3293 - mean_absolute_error: 7.2133 - val_loss: 153.4749 - val_mean_absolute_error: 10.1245\n",
            "Epoch 141/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 86.2542 - mean_absolute_error: 7.2462 - val_loss: 81.0263 - val_mean_absolute_error: 6.9968\n",
            "Epoch 142/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 87.0147 - mean_absolute_error: 7.1188 - val_loss: 67.4247 - val_mean_absolute_error: 6.4366\n",
            "Epoch 143/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 103.7353 - mean_absolute_error: 7.8607 - val_loss: 88.8274 - val_mean_absolute_error: 7.3590\n",
            "Epoch 144/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 89.4709 - mean_absolute_error: 7.1418 - val_loss: 79.9241 - val_mean_absolute_error: 6.9319\n",
            "Epoch 145/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 95.2512 - mean_absolute_error: 7.6148 - val_loss: 71.5288 - val_mean_absolute_error: 6.5701\n",
            "Epoch 146/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 98.3791 - mean_absolute_error: 7.3849 - val_loss: 93.1117 - val_mean_absolute_error: 7.6416\n",
            "Epoch 147/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 79.8667 - mean_absolute_error: 6.8336 - val_loss: 68.5982 - val_mean_absolute_error: 6.3864\n",
            "Epoch 148/400\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 96.7874 - mean_absolute_error: 7.5266 - val_loss: 77.4025 - val_mean_absolute_error: 6.5491\n",
            "Epoch 149/400\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 87.6278 - mean_absolute_error: 7.1730 - val_loss: 71.6308 - val_mean_absolute_error: 6.6874\n",
            "Epoch 150/400\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 87.7824 - mean_absolute_error: 7.2770 - val_loss: 114.5745 - val_mean_absolute_error: 8.4598\n",
            "Epoch 151/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 74.9146 - mean_absolute_error: 6.6115 - val_loss: 78.8788 - val_mean_absolute_error: 6.4090\n",
            "Epoch 152/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 111.6079 - mean_absolute_error: 8.0169 - val_loss: 114.8376 - val_mean_absolute_error: 8.4609\n",
            "Epoch 153/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 111.0411 - mean_absolute_error: 8.1048 - val_loss: 80.6867 - val_mean_absolute_error: 7.0422\n",
            "Epoch 154/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 77.8494 - mean_absolute_error: 6.6243 - val_loss: 102.6966 - val_mean_absolute_error: 8.0293\n",
            "Epoch 155/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 110.2063 - mean_absolute_error: 8.0708 - val_loss: 181.0368 - val_mean_absolute_error: 10.7535\n",
            "Epoch 156/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 94.0124 - mean_absolute_error: 7.3577 - val_loss: 75.7468 - val_mean_absolute_error: 6.8554\n",
            "Epoch 157/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 95.9632 - mean_absolute_error: 7.5407 - val_loss: 68.1720 - val_mean_absolute_error: 6.3945\n",
            "Epoch 158/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 79.5123 - mean_absolute_error: 6.7110 - val_loss: 64.6263 - val_mean_absolute_error: 6.1825\n",
            "Epoch 159/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 94.7847 - mean_absolute_error: 7.5538 - val_loss: 72.1368 - val_mean_absolute_error: 6.7693\n",
            "Epoch 160/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 79.6742 - mean_absolute_error: 6.8220 - val_loss: 76.9172 - val_mean_absolute_error: 6.8852\n",
            "Epoch 161/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 82.8097 - mean_absolute_error: 6.9758 - val_loss: 77.4449 - val_mean_absolute_error: 6.8552\n",
            "Epoch 162/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 81.7185 - mean_absolute_error: 6.9936 - val_loss: 67.9423 - val_mean_absolute_error: 6.3173\n",
            "Epoch 163/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 95.2161 - mean_absolute_error: 7.4314 - val_loss: 66.0219 - val_mean_absolute_error: 6.1714\n",
            "Epoch 164/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 84.2337 - mean_absolute_error: 7.0623 - val_loss: 130.4031 - val_mean_absolute_error: 9.1709\n",
            "Epoch 165/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 96.1301 - mean_absolute_error: 7.6406 - val_loss: 75.1572 - val_mean_absolute_error: 6.5756\n",
            "Epoch 166/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 90.8822 - mean_absolute_error: 7.3007 - val_loss: 71.8147 - val_mean_absolute_error: 6.4213\n",
            "Epoch 167/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 95.6829 - mean_absolute_error: 7.4490 - val_loss: 76.6798 - val_mean_absolute_error: 6.8890\n",
            "Epoch 168/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 79.8500 - mean_absolute_error: 6.8143 - val_loss: 69.9543 - val_mean_absolute_error: 6.3303\n",
            "Epoch 169/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 100.4717 - mean_absolute_error: 7.7945 - val_loss: 75.5240 - val_mean_absolute_error: 6.7738\n",
            "Epoch 170/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 86.0551 - mean_absolute_error: 7.2111 - val_loss: 78.4871 - val_mean_absolute_error: 6.6796\n",
            "Epoch 171/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 94.0441 - mean_absolute_error: 7.4841 - val_loss: 89.8441 - val_mean_absolute_error: 7.4283\n",
            "Epoch 172/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 93.7462 - mean_absolute_error: 7.3516 - val_loss: 70.1409 - val_mean_absolute_error: 6.3523\n",
            "Epoch 173/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 81.2928 - mean_absolute_error: 6.9365 - val_loss: 68.3237 - val_mean_absolute_error: 6.2432\n",
            "Epoch 174/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 79.1903 - mean_absolute_error: 6.7669 - val_loss: 73.3840 - val_mean_absolute_error: 6.5133\n",
            "Epoch 175/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 97.6811 - mean_absolute_error: 7.6154 - val_loss: 86.6713 - val_mean_absolute_error: 7.3507\n",
            "Epoch 176/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 99.5013 - mean_absolute_error: 7.7345 - val_loss: 198.7461 - val_mean_absolute_error: 11.4874\n",
            "Epoch 177/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 93.9879 - mean_absolute_error: 7.3949 - val_loss: 172.7180 - val_mean_absolute_error: 10.7797\n",
            "Epoch 178/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 100.3142 - mean_absolute_error: 7.8225 - val_loss: 154.0916 - val_mean_absolute_error: 10.0688\n",
            "Epoch 179/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 100.5518 - mean_absolute_error: 7.6879 - val_loss: 81.7405 - val_mean_absolute_error: 7.1445\n",
            "Epoch 180/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 85.6546 - mean_absolute_error: 6.9976 - val_loss: 66.8736 - val_mean_absolute_error: 6.4780\n",
            "Epoch 181/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 82.8686 - mean_absolute_error: 6.9219 - val_loss: 108.6332 - val_mean_absolute_error: 8.3126\n",
            "Epoch 182/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 92.8975 - mean_absolute_error: 7.3922 - val_loss: 80.3044 - val_mean_absolute_error: 6.9598\n",
            "Epoch 183/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 100.0601 - mean_absolute_error: 7.4972 - val_loss: 81.5902 - val_mean_absolute_error: 7.0529\n",
            "Epoch 184/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 86.4823 - mean_absolute_error: 7.1910 - val_loss: 72.5403 - val_mean_absolute_error: 6.5997\n",
            "Epoch 185/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 84.9483 - mean_absolute_error: 6.8384 - val_loss: 73.6686 - val_mean_absolute_error: 6.8708\n",
            "Epoch 186/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 84.9232 - mean_absolute_error: 7.1048 - val_loss: 70.0724 - val_mean_absolute_error: 6.6789\n",
            "Epoch 187/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 90.3869 - mean_absolute_error: 7.3987 - val_loss: 74.2260 - val_mean_absolute_error: 6.8101\n",
            "Epoch 188/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 92.2255 - mean_absolute_error: 7.3274 - val_loss: 68.2613 - val_mean_absolute_error: 6.4496\n",
            "Epoch 189/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 84.2415 - mean_absolute_error: 7.0828 - val_loss: 87.2509 - val_mean_absolute_error: 7.3311\n",
            "Epoch 190/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 92.4436 - mean_absolute_error: 7.3115 - val_loss: 63.9715 - val_mean_absolute_error: 6.2789\n",
            "Epoch 191/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 87.6840 - mean_absolute_error: 7.1551 - val_loss: 111.5615 - val_mean_absolute_error: 8.4411\n",
            "Epoch 192/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 94.9014 - mean_absolute_error: 7.4718 - val_loss: 66.8644 - val_mean_absolute_error: 6.2555\n",
            "Epoch 193/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 84.6641 - mean_absolute_error: 7.0213 - val_loss: 79.7790 - val_mean_absolute_error: 6.7134\n",
            "Epoch 194/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 96.2068 - mean_absolute_error: 7.3934 - val_loss: 66.7235 - val_mean_absolute_error: 6.2104\n",
            "Epoch 195/400\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 80.8249 - mean_absolute_error: 7.0015 - val_loss: 88.6858 - val_mean_absolute_error: 7.4524\n",
            "Epoch 196/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 82.8661 - mean_absolute_error: 7.0111 - val_loss: 69.6164 - val_mean_absolute_error: 6.1303\n",
            "Epoch 197/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 83.4703 - mean_absolute_error: 6.8248 - val_loss: 88.4936 - val_mean_absolute_error: 7.4435\n",
            "Epoch 198/400\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 101.4783 - mean_absolute_error: 7.8501 - val_loss: 80.1009 - val_mean_absolute_error: 7.0931\n",
            "Epoch 199/400\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 93.0707 - mean_absolute_error: 7.3978 - val_loss: 84.5499 - val_mean_absolute_error: 7.2942\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 89.0349 - mean_absolute_error: 7.5773\n",
            "Final training loss:  93.07071685791016\n",
            "Final validation loss:  84.54989624023438\n",
            "Final training mae loss:  7.397763252258301\n",
            "Final validation mae loss:  7.294164180755615\n",
            "WARNING:tensorflow:6 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fea899bb950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Predictions:  [[21.823412]\n",
            " [21.6737  ]\n",
            " [43.33909 ]\n",
            " [21.292746]\n",
            " [26.936167]]\n",
            "True labels:  [21.54 25.37 49.2  19.69 26.97]\n",
            "Tester: Regression model learning OK\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTBqUmDzPiYI"
      },
      "source": [
        "## **F**: A regressziós adatbázis átalakítása klasszifikációs adatbázissá\n",
        "\n",
        "Alakítsd át a regressziós adatbázist, hogy alkalmas legyen \n",
        "klasszifikációs feladathoz! A kategóriákat az alábbiak alapján alakítsd ki : \n",
        "\n",
        "*   **F/1**: 3 kategória. A kategóriákat úgy alakítsd ki, hogy a regressziós adatbázis címkéinek intervallumán felveszel két küszöbértéket. Az első kategóriába a kisebb küszöbérték alatti, a másodikba a két küszöbérték közti és a harmadikba a nagyobb küszöbérték feletti címkével rendelkező mintaelemek kerüljenek. \n",
        "Tehát, például ha a regressziós adatbázis címkéi 3 és 8 közt vannak, akkor a két küszöbértéknek választhatjuk az 5.2-t és a 6.1-et. Ekkor az 1. kategóriába az 5.2-nél kisebb címkéjű elemek, a 2. kategóriába az 5.2 és 6.1 közti címkéjű elemek és a 3. kategóriába a 6.1-nél nagyobb címkéjű elemek kerülnek. Azonos címkéjű elemek mindenképpen azonos kategóriába kell, hogy kerüljenek.\n",
        "\n",
        "*   **F/2**: 4 kategória. A kategóriákat úgy alakítsd ki, hogy a regressziós adatbázis címkéinek intervallumán felveszel három küszöbértéket. Az első kategóriába a kisebb küszöbérték alatti, a másodikba az első és második küszöbérték közti, a harmadikba a második és a haramdik küszöbérték közti, és a negyedikbe a harmadik küszöbérték feletti címkével rendelkező mintaelemek kerüljenek. \n",
        "Tehát, például ha a regressziós adatbázis címkéi 3 és 8 közt vannak, akkor a három küszöbértéknek választhatjuk a 3.8-at, a 6.1-et és a 7.5-öt. Ekkor az 1. kategóriába az 3.8-nál kisebb címkéjű elemek, a 2. kategóriába az 3.8 és 6.1 közti címkéjű elemek, a 3. kategóriába az 6.1 és 7.5 közti címkéjű elemek és a 4. kategóriába a 7.5-nél nagyobb címkéjű elemek kerülnek. Azonos címkéjű elemek mindenképpen azonos kategóriába kell, hogy kerüljenek.\n",
        "\n",
        "\n",
        "Válaszd ki a küszöbértékeket úgy, hogy a kategóriábákba **megközelítőleg** ugyanannyi elem kerüljön! Hozd létre az új kategóriacímkéket az `y_train`, `y_val`, `y_test` tömbökből. Kerüljenek ezek a `y_cat_train`, `y_cat_val`, `y_cat_test` változókba!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCdOectqRsvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3583a852-322e-4f56-e170-27aed87dd970"
      },
      "source": [
        "# implement your solution BELOW\n",
        "y_cat_train = np.where(y_train > 41.9, 2, (np.where(y_train < 27.9, 0, 1)))\n",
        "y_cat_val = np.where(y_val > 41.9, 2, (np.where(y_val < 27.9, 0, 1)))\n",
        "y_cat_test = np.where(y_test > 41.9, 2, (np.where(y_test < 27.9, 0, 1)))\n",
        "\n",
        "# implement your solution ABOVE\n",
        "\n",
        "tester.test('cl_dataset', y_cat_train, y_cat_val, y_cat_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tester: Classification dataset creation OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_OwgkhPxCuA"
      },
      "source": [
        "## **G**: Kategóriacímkék átalakítása one-hot kódolásra\n",
        "\n",
        "Kettőnél több kategóriás (multi-class) klasszifikációnál minden kategóriához egy valószínűséget becslünk, azaz a háló kimenete akkora méretű, ahány kategóriánk van. Így tehát az igazi címkéink ugyanekkora méretű one-hot kódolású vektorok lesznek, ezeket címkevektorokként használva fogjuk betanítani a hálót.\n",
        "\n",
        "Készítsd el a one-hot kódolású címkevektor tömböket a kategóriacímkéket tartalmazó `y_cat_train`, `y_cat_val`, `y_cat_test` tömbökből! Az új tömbök kerüljenek a `y_onehot_train`, `y_onehot_val`, `y_onehot_test` változókba! A tömbök alakja így (n_samples, n_categories), típusuk legyen `np.float32`!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLAlR3qyxT13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "147254ab-a8ee-4024-b394-78e3234beb17"
      },
      "source": [
        "# implement your solution BELOW\n",
        "y_onehot_train = np.zeros((y_cat_train.size, y_cat_train.max()+1))\n",
        "y_onehot_train[np.arange(y_cat_train.size), y_cat_train] = 1\n",
        "\n",
        "y_onehot_val = np.zeros((y_cat_val.size, y_cat_val.max()+1))\n",
        "y_onehot_val[np.arange(y_cat_val.size), y_cat_val] = 1\n",
        "\n",
        "y_onehot_test = np.zeros((y_cat_test.size, y_cat_test.max()+1))\n",
        "y_onehot_test[np.arange(y_cat_test.size), y_cat_test] = 1\n",
        "\n",
        "# implement your solution ABOVE\n",
        "\n",
        "tester.test('cl_onehot', y_onehot_train, y_onehot_val, y_onehot_test)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tester: One-hot conversion OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xdtHfyrwexF"
      },
      "source": [
        "## **H**: A klasszifikációs neuronháló definiálása Keras-ban \n",
        "\n",
        "Használd a `keras.models.Sequential()` osztályt a háló definiálásához! A háló architektúráját az alábbiak alapján alakítsd ki:\n",
        "- **H/1**: Kétrétegű háló, az első réteg neuronjainak száma 50, aktivációs függvénye ReLU. Az első réteg után alkalmazz dropout-ot neurononként 30%-os valószínűséggel!\n",
        "- **H/2**: Négyrétegű háló, a neuronok száma rendre 20, 20, 10, aktivációs függvényük ReLU. Az egyes rétegek közé tégy dropout rétegeket, melyek 20%-os valószínűséggel nulláznak ki egy-egy elemet!\n",
        "- **H/3**: Háromrétegű háló, a neuronok száma rendre 50, 30. Aktivációs függvényként használj sorban tanh-t, majd ReLU-t!\n",
        "\n",
        "Minden esetben csak teljesen összekötött (dense) rétegeket, valamint dropout rétegeket kell használnod. Az aktivációs függvényeket a telejesen összekötött rétegek `activation` paramétereként add meg (azaz most ne használj külön Acitvation rétegeket). A rétegek számába beleértendő a kimenetre képző, utolsó réteg is, melynek méretét a címke mérete határozza meg. Alkalmazd a tanult aktivációs függvényt az utolsó rétegen, függően a feladat típusától!\n",
        "A modellt fordítsd le a `compile()` függvény segítségével: a költségfüggvény legyen a szokásos, multi-class klasszifikációhoz használatos (categorical) crossentropy, optimizer algoritmusként pedig alkalmazd az SGD algoritmust megfelelő tanulási rátával! \n",
        "A klasszifikációs neuronhálót tartalmazó keras.models.Sequential() típusú változót nevezd el `cl_model`-nek. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Iwot3xxVXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d81725a-6b2d-419e-e073-912de5392c99"
      },
      "source": [
        "# implement your solution BELOW\n",
        "tensorflow.random.set_seed(42)\n",
        "\n",
        "cl_model = tensorflow.keras.models.Sequential()\n",
        "cl_model.add(tensorflow.keras.layers.Dense(50, activation=\"relu\", input_dim=x_train.shape[1]))\n",
        "cl_model.add(tensorflow.keras.layers.Dropout(0.3))\n",
        "cl_model.add(tensorflow.keras.layers.Dense(3, activation=\"softmax\"))\n",
        "\n",
        "cl_model.compile(optimizer=tensorflow.keras.optimizers.SGD(learning_rate=0.01), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# implement your solution ABOVE\n",
        "\n",
        "tester.test('cl_model_architecture', cl_model)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tester: Classification model architecture OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uowunGxa2Mq"
      },
      "source": [
        "## **I**: A háló betanítása (multi-class) klasszifikációs feladatra\n",
        "\n",
        "Tanítsd be a neuronhálót a tanítóhalmazon! Használj early stopping-ot a validációs halmazzal! A tanulási rátát, epoch-ok maximális számát, a batch méretet, az early stopping `patience` paraméterét szabadon átállíthatod. Próbálgathatsz különböző kombinációkat, hogy jobb eredményt érj el.\n",
        "\n",
        "- Rajzold ki, hogyan alakult a tanulási és validációs költség a betanítás során! Ehhez használhatod a `matplotlib` könyvtárat, példát találsz a 8. előadás notebookjában. \n",
        "- A crossentropy költség mellett, a helyesen klasszifikált mintaelemek arányának (_accuracy_) tanító- és validációs halmazon történő alakulását is rajzold ki egy másik grafikonon! A grafikonok y tengelyének megjelenített értékhatárait úgy állítsd be, hogy mindegyik idősor látható legyen és könnyen ki lehessen venni a költségek alakulását a tanítás vége felé is!\n",
        "- Számold ki a betanított modell crossentropy költségét és az accuracy metrikát a teszthalamzon, majd add értékül ezeket az értékeket a `test_ce` és `test_acc` változóknak!\n",
        "- Számold ki a multiclass _Fals pozitív arány_ (FPR) és _Fals negatív arány_ (FNR) metrikákat a teszthalmazon a betanított modelledre, majd add értékül ezeket az értékeket a `test_fpr` és `test_fnr` változóknak! Az FPR és FNR kiszámításához az első házi feladat B/1. és B/2. feladataiban található képletek és leírás segíthetnek. A **saját**, 1. házi feladatban beadott FPR vagy FNR metrikát kiszámító kódodat természetesen felhasználhatod itt is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHWZ8AN2oGoP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1dedb1b0-628f-4d11-ab1b-65d4a4034721"
      },
      "source": [
        "# implement your solution BELOW\n",
        "def false_positive_rate(y_pred, y_true):\n",
        "  prediction_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "  one_hot_encoding_prediction_labels = np.zeros((prediction_labels.size, prediction_labels.max()+1))\n",
        "  one_hot_encoding_prediction_labels[np.arange(prediction_labels.size), prediction_labels] = 1\n",
        "\n",
        "  fp_values = np.count_nonzero(np.logical_and(one_hot_encoding_prediction_labels == 1., y_true == 0.), axis=0)\n",
        "  vn_values = np.count_nonzero(np.logical_and(one_hot_encoding_prediction_labels == 0., y_true == 0.), axis=0)\n",
        "  fpr = np.sum(np.where(fp_values == 0, 0, fp_values / (fp_values + vn_values))) / fp_values.size\n",
        "\n",
        "  return fpr\n",
        "\n",
        "def false_negative_rate(y_pred, y_true):\n",
        "  prediction_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "  one_hot_encoding_prediction_labels = np.zeros((prediction_labels.size, prediction_labels.max()+1))\n",
        "  one_hot_encoding_prediction_labels[np.arange(prediction_labels.size), prediction_labels] = 1\n",
        "\n",
        "  fn_values = np.count_nonzero(np.logical_and(one_hot_encoding_prediction_labels == 0., y_true == 1.), axis=0)\n",
        "  tp_values = np.count_nonzero(np.logical_and(one_hot_encoding_prediction_labels == 1., y_true == 1.), axis=0)\n",
        "  fnr = np.sum(np.where(fn_values == 0, 0, fn_values / (fn_values + tp_values))) / fn_values.size\n",
        "\n",
        "  return fnr\n",
        "\n",
        "earlystopping_callback = tensorflow.keras.callbacks.EarlyStopping(monitor=\"accuracy\", patience=110)\n",
        "\n",
        "history = cl_model.fit(x_train, y_onehot_train, validation_data=(x_val, y_onehot_val), batch_size=64, epochs=1000, verbose=1, callbacks=[earlystopping_callback])\n",
        "\n",
        "tr_losses = history.history['loss']\n",
        "val_losses = history.history['val_loss']\n",
        "\n",
        "tr_accuracies = history.history['accuracy']\n",
        "val_accuracies = history.history['val_accuracy']\n",
        "\n",
        "print(\"Final training loss: \", tr_losses[-1])\n",
        "print(\"Final validation loss: \", val_losses[-1])\n",
        "print(\"Final training accuracy: \", tr_accuracies[-1])\n",
        "print(\"Final validation accuracy: \", val_accuracies[-1])\n",
        "\n",
        "predictions = cl_model.predict(x_test)\n",
        "\n",
        "test_ce, test_acc = cl_model.evaluate(x_test, y_onehot_test)\n",
        "test_fpr = false_positive_rate(predictions, y_onehot_test)\n",
        "test_fnr = false_negative_rate(predictions, y_onehot_test)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True)\n",
        "fig.suptitle('Crossentropy loss and Accuracy')\n",
        "\n",
        "ax1.plot(tr_losses, label=\"J_train\")\n",
        "ax1.plot(val_losses, label=\"J_val\")\n",
        "ax1.set(xlabel=\"Number of epochs\", ylabel=\"Cost (J)\")\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(tr_accuracies, label=\"A_train\")\n",
        "ax2.plot(val_accuracies, label=\"A_val\")\n",
        "ax2.set(xlabel=\"Number of epochs\", ylabel=\"Accuracy (A)\")\n",
        "ax2.legend()\n",
        "\n",
        "# implement your solution ABOVE\n",
        "\n",
        "tester.test('cl_model_learning', test_ce, test_acc, test_fpr, test_fnr)\n",
        "tester.print_all_tests_successful()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "12/12 [==============================] - 1s 21ms/step - loss: 1.1170 - accuracy: 0.3167 - val_loss: 1.1069 - val_accuracy: 0.3226\n",
            "Epoch 2/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1102 - accuracy: 0.3194 - val_loss: 1.1058 - val_accuracy: 0.3290\n",
            "Epoch 3/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1117 - accuracy: 0.3139 - val_loss: 1.1050 - val_accuracy: 0.3290\n",
            "Epoch 4/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1137 - accuracy: 0.3250 - val_loss: 1.1040 - val_accuracy: 0.3548\n",
            "Epoch 5/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1034 - accuracy: 0.3278 - val_loss: 1.1029 - val_accuracy: 0.3806\n",
            "Epoch 6/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1113 - accuracy: 0.3250 - val_loss: 1.1018 - val_accuracy: 0.4065\n",
            "Epoch 7/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0996 - accuracy: 0.3625 - val_loss: 1.1008 - val_accuracy: 0.3871\n",
            "Epoch 8/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1032 - accuracy: 0.3292 - val_loss: 1.0998 - val_accuracy: 0.3742\n",
            "Epoch 9/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1031 - accuracy: 0.3361 - val_loss: 1.0988 - val_accuracy: 0.4000\n",
            "Epoch 10/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1025 - accuracy: 0.3458 - val_loss: 1.0975 - val_accuracy: 0.4000\n",
            "Epoch 11/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0990 - accuracy: 0.3764 - val_loss: 1.0966 - val_accuracy: 0.4129\n",
            "Epoch 12/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0964 - accuracy: 0.3500 - val_loss: 1.0956 - val_accuracy: 0.4194\n",
            "Epoch 13/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0957 - accuracy: 0.3764 - val_loss: 1.0945 - val_accuracy: 0.4258\n",
            "Epoch 14/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0884 - accuracy: 0.3792 - val_loss: 1.0937 - val_accuracy: 0.4258\n",
            "Epoch 15/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0938 - accuracy: 0.3681 - val_loss: 1.0925 - val_accuracy: 0.4194\n",
            "Epoch 16/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0895 - accuracy: 0.3861 - val_loss: 1.0917 - val_accuracy: 0.4129\n",
            "Epoch 17/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0823 - accuracy: 0.3958 - val_loss: 1.0907 - val_accuracy: 0.4065\n",
            "Epoch 18/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0854 - accuracy: 0.4194 - val_loss: 1.0894 - val_accuracy: 0.4129\n",
            "Epoch 19/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0872 - accuracy: 0.4125 - val_loss: 1.0880 - val_accuracy: 0.4258\n",
            "Epoch 20/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0822 - accuracy: 0.4056 - val_loss: 1.0871 - val_accuracy: 0.4258\n",
            "Epoch 21/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0927 - accuracy: 0.3583 - val_loss: 1.0866 - val_accuracy: 0.4000\n",
            "Epoch 22/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0754 - accuracy: 0.4250 - val_loss: 1.0855 - val_accuracy: 0.3935\n",
            "Epoch 23/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0768 - accuracy: 0.4181 - val_loss: 1.0842 - val_accuracy: 0.4065\n",
            "Epoch 24/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0814 - accuracy: 0.4000 - val_loss: 1.0832 - val_accuracy: 0.4258\n",
            "Epoch 25/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0793 - accuracy: 0.4125 - val_loss: 1.0822 - val_accuracy: 0.4258\n",
            "Epoch 26/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0690 - accuracy: 0.4542 - val_loss: 1.0808 - val_accuracy: 0.4387\n",
            "Epoch 27/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0749 - accuracy: 0.3972 - val_loss: 1.0797 - val_accuracy: 0.4452\n",
            "Epoch 28/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0742 - accuracy: 0.4194 - val_loss: 1.0787 - val_accuracy: 0.4516\n",
            "Epoch 29/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0697 - accuracy: 0.4236 - val_loss: 1.0775 - val_accuracy: 0.4581\n",
            "Epoch 30/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0684 - accuracy: 0.4292 - val_loss: 1.0766 - val_accuracy: 0.4516\n",
            "Epoch 31/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0677 - accuracy: 0.4444 - val_loss: 1.0756 - val_accuracy: 0.4581\n",
            "Epoch 32/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0622 - accuracy: 0.4556 - val_loss: 1.0747 - val_accuracy: 0.4645\n",
            "Epoch 33/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0662 - accuracy: 0.4208 - val_loss: 1.0736 - val_accuracy: 0.4710\n",
            "Epoch 34/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0590 - accuracy: 0.4694 - val_loss: 1.0725 - val_accuracy: 0.4710\n",
            "Epoch 35/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0585 - accuracy: 0.4750 - val_loss: 1.0716 - val_accuracy: 0.4645\n",
            "Epoch 36/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0664 - accuracy: 0.4528 - val_loss: 1.0705 - val_accuracy: 0.4774\n",
            "Epoch 37/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0565 - accuracy: 0.4750 - val_loss: 1.0696 - val_accuracy: 0.4839\n",
            "Epoch 38/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0556 - accuracy: 0.4681 - val_loss: 1.0683 - val_accuracy: 0.4839\n",
            "Epoch 39/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0629 - accuracy: 0.4222 - val_loss: 1.0672 - val_accuracy: 0.4903\n",
            "Epoch 40/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0564 - accuracy: 0.4514 - val_loss: 1.0662 - val_accuracy: 0.4903\n",
            "Epoch 41/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0597 - accuracy: 0.4472 - val_loss: 1.0650 - val_accuracy: 0.4968\n",
            "Epoch 42/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0571 - accuracy: 0.4833 - val_loss: 1.0641 - val_accuracy: 0.5032\n",
            "Epoch 43/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0565 - accuracy: 0.4694 - val_loss: 1.0626 - val_accuracy: 0.5226\n",
            "Epoch 44/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0540 - accuracy: 0.4667 - val_loss: 1.0617 - val_accuracy: 0.5226\n",
            "Epoch 45/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0516 - accuracy: 0.4569 - val_loss: 1.0607 - val_accuracy: 0.5161\n",
            "Epoch 46/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0433 - accuracy: 0.4917 - val_loss: 1.0598 - val_accuracy: 0.5161\n",
            "Epoch 47/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0527 - accuracy: 0.4583 - val_loss: 1.0586 - val_accuracy: 0.5226\n",
            "Epoch 48/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0484 - accuracy: 0.4806 - val_loss: 1.0575 - val_accuracy: 0.5161\n",
            "Epoch 49/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0407 - accuracy: 0.5042 - val_loss: 1.0559 - val_accuracy: 0.5097\n",
            "Epoch 50/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0437 - accuracy: 0.5083 - val_loss: 1.0548 - val_accuracy: 0.5097\n",
            "Epoch 51/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0438 - accuracy: 0.4875 - val_loss: 1.0537 - val_accuracy: 0.5032\n",
            "Epoch 52/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0440 - accuracy: 0.4778 - val_loss: 1.0529 - val_accuracy: 0.5097\n",
            "Epoch 53/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0345 - accuracy: 0.4833 - val_loss: 1.0516 - val_accuracy: 0.5097\n",
            "Epoch 54/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0348 - accuracy: 0.4958 - val_loss: 1.0504 - val_accuracy: 0.5032\n",
            "Epoch 55/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0321 - accuracy: 0.4931 - val_loss: 1.0494 - val_accuracy: 0.5032\n",
            "Epoch 56/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0364 - accuracy: 0.4944 - val_loss: 1.0484 - val_accuracy: 0.5032\n",
            "Epoch 57/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0382 - accuracy: 0.4861 - val_loss: 1.0473 - val_accuracy: 0.5032\n",
            "Epoch 58/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0378 - accuracy: 0.4667 - val_loss: 1.0464 - val_accuracy: 0.5032\n",
            "Epoch 59/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0322 - accuracy: 0.5222 - val_loss: 1.0453 - val_accuracy: 0.5032\n",
            "Epoch 60/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0318 - accuracy: 0.5125 - val_loss: 1.0447 - val_accuracy: 0.5161\n",
            "Epoch 61/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0361 - accuracy: 0.4917 - val_loss: 1.0439 - val_accuracy: 0.5290\n",
            "Epoch 62/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0242 - accuracy: 0.5250 - val_loss: 1.0427 - val_accuracy: 0.5290\n",
            "Epoch 63/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0279 - accuracy: 0.4931 - val_loss: 1.0414 - val_accuracy: 0.5161\n",
            "Epoch 64/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0214 - accuracy: 0.5125 - val_loss: 1.0400 - val_accuracy: 0.5032\n",
            "Epoch 65/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0300 - accuracy: 0.4972 - val_loss: 1.0387 - val_accuracy: 0.4968\n",
            "Epoch 66/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0270 - accuracy: 0.4944 - val_loss: 1.0375 - val_accuracy: 0.4968\n",
            "Epoch 67/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0185 - accuracy: 0.5028 - val_loss: 1.0361 - val_accuracy: 0.4968\n",
            "Epoch 68/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0155 - accuracy: 0.5153 - val_loss: 1.0353 - val_accuracy: 0.5032\n",
            "Epoch 69/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0166 - accuracy: 0.5028 - val_loss: 1.0340 - val_accuracy: 0.5032\n",
            "Epoch 70/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0172 - accuracy: 0.5167 - val_loss: 1.0330 - val_accuracy: 0.5032\n",
            "Epoch 71/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0139 - accuracy: 0.5250 - val_loss: 1.0315 - val_accuracy: 0.5032\n",
            "Epoch 72/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0119 - accuracy: 0.5000 - val_loss: 1.0299 - val_accuracy: 0.5097\n",
            "Epoch 73/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0160 - accuracy: 0.4875 - val_loss: 1.0293 - val_accuracy: 0.5032\n",
            "Epoch 74/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0148 - accuracy: 0.5222 - val_loss: 1.0282 - val_accuracy: 0.5097\n",
            "Epoch 75/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0112 - accuracy: 0.5347 - val_loss: 1.0273 - val_accuracy: 0.5097\n",
            "Epoch 76/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0110 - accuracy: 0.5194 - val_loss: 1.0262 - val_accuracy: 0.5161\n",
            "Epoch 77/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0053 - accuracy: 0.5264 - val_loss: 1.0248 - val_accuracy: 0.5161\n",
            "Epoch 78/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9977 - accuracy: 0.5194 - val_loss: 1.0237 - val_accuracy: 0.5097\n",
            "Epoch 79/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0058 - accuracy: 0.5222 - val_loss: 1.0223 - val_accuracy: 0.5161\n",
            "Epoch 80/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0007 - accuracy: 0.5319 - val_loss: 1.0212 - val_accuracy: 0.5161\n",
            "Epoch 81/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9946 - accuracy: 0.5528 - val_loss: 1.0201 - val_accuracy: 0.5161\n",
            "Epoch 82/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0010 - accuracy: 0.5417 - val_loss: 1.0187 - val_accuracy: 0.5161\n",
            "Epoch 83/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0033 - accuracy: 0.5347 - val_loss: 1.0174 - val_accuracy: 0.5161\n",
            "Epoch 84/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0000 - accuracy: 0.5264 - val_loss: 1.0162 - val_accuracy: 0.5226\n",
            "Epoch 85/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9877 - accuracy: 0.5458 - val_loss: 1.0150 - val_accuracy: 0.5226\n",
            "Epoch 86/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0016 - accuracy: 0.5139 - val_loss: 1.0142 - val_accuracy: 0.5097\n",
            "Epoch 87/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9909 - accuracy: 0.5431 - val_loss: 1.0129 - val_accuracy: 0.5032\n",
            "Epoch 88/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9901 - accuracy: 0.5472 - val_loss: 1.0120 - val_accuracy: 0.5032\n",
            "Epoch 89/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9937 - accuracy: 0.5417 - val_loss: 1.0108 - val_accuracy: 0.5097\n",
            "Epoch 90/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9806 - accuracy: 0.5444 - val_loss: 1.0092 - val_accuracy: 0.5097\n",
            "Epoch 91/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9901 - accuracy: 0.5417 - val_loss: 1.0080 - val_accuracy: 0.5097\n",
            "Epoch 92/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9911 - accuracy: 0.5333 - val_loss: 1.0070 - val_accuracy: 0.5097\n",
            "Epoch 93/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9789 - accuracy: 0.5625 - val_loss: 1.0052 - val_accuracy: 0.5097\n",
            "Epoch 94/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9760 - accuracy: 0.5597 - val_loss: 1.0041 - val_accuracy: 0.5097\n",
            "Epoch 95/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9781 - accuracy: 0.5528 - val_loss: 1.0034 - val_accuracy: 0.5097\n",
            "Epoch 96/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9842 - accuracy: 0.5444 - val_loss: 1.0026 - val_accuracy: 0.5097\n",
            "Epoch 97/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9793 - accuracy: 0.5458 - val_loss: 1.0013 - val_accuracy: 0.5032\n",
            "Epoch 98/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9781 - accuracy: 0.5319 - val_loss: 0.9995 - val_accuracy: 0.5097\n",
            "Epoch 99/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9704 - accuracy: 0.5542 - val_loss: 0.9990 - val_accuracy: 0.5032\n",
            "Epoch 100/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9789 - accuracy: 0.5528 - val_loss: 0.9984 - val_accuracy: 0.5032\n",
            "Epoch 101/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9750 - accuracy: 0.5472 - val_loss: 0.9969 - val_accuracy: 0.5032\n",
            "Epoch 102/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9678 - accuracy: 0.5528 - val_loss: 0.9956 - val_accuracy: 0.5032\n",
            "Epoch 103/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9810 - accuracy: 0.5153 - val_loss: 0.9943 - val_accuracy: 0.5032\n",
            "Epoch 104/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9639 - accuracy: 0.5639 - val_loss: 0.9932 - val_accuracy: 0.5032\n",
            "Epoch 105/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9643 - accuracy: 0.5694 - val_loss: 0.9917 - val_accuracy: 0.5032\n",
            "Epoch 106/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9611 - accuracy: 0.5639 - val_loss: 0.9909 - val_accuracy: 0.5097\n",
            "Epoch 107/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9704 - accuracy: 0.5458 - val_loss: 0.9894 - val_accuracy: 0.5097\n",
            "Epoch 108/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9592 - accuracy: 0.5806 - val_loss: 0.9879 - val_accuracy: 0.5097\n",
            "Epoch 109/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9596 - accuracy: 0.5597 - val_loss: 0.9868 - val_accuracy: 0.5097\n",
            "Epoch 110/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9590 - accuracy: 0.5667 - val_loss: 0.9860 - val_accuracy: 0.5097\n",
            "Epoch 111/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9682 - accuracy: 0.5556 - val_loss: 0.9854 - val_accuracy: 0.5032\n",
            "Epoch 112/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9544 - accuracy: 0.5722 - val_loss: 0.9836 - val_accuracy: 0.5161\n",
            "Epoch 113/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9626 - accuracy: 0.5889 - val_loss: 0.9829 - val_accuracy: 0.5161\n",
            "Epoch 114/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9641 - accuracy: 0.5403 - val_loss: 0.9812 - val_accuracy: 0.5226\n",
            "Epoch 115/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9567 - accuracy: 0.5694 - val_loss: 0.9803 - val_accuracy: 0.5290\n",
            "Epoch 116/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9478 - accuracy: 0.5847 - val_loss: 0.9794 - val_accuracy: 0.5226\n",
            "Epoch 117/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9452 - accuracy: 0.5597 - val_loss: 0.9784 - val_accuracy: 0.5226\n",
            "Epoch 118/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9516 - accuracy: 0.5597 - val_loss: 0.9775 - val_accuracy: 0.5226\n",
            "Epoch 119/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9537 - accuracy: 0.5653 - val_loss: 0.9769 - val_accuracy: 0.5097\n",
            "Epoch 120/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9527 - accuracy: 0.5403 - val_loss: 0.9756 - val_accuracy: 0.5161\n",
            "Epoch 121/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9418 - accuracy: 0.5764 - val_loss: 0.9745 - val_accuracy: 0.5226\n",
            "Epoch 122/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9514 - accuracy: 0.5653 - val_loss: 0.9734 - val_accuracy: 0.5226\n",
            "Epoch 123/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9487 - accuracy: 0.5708 - val_loss: 0.9721 - val_accuracy: 0.5226\n",
            "Epoch 124/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9413 - accuracy: 0.5708 - val_loss: 0.9707 - val_accuracy: 0.5290\n",
            "Epoch 125/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9539 - accuracy: 0.5528 - val_loss: 0.9705 - val_accuracy: 0.5226\n",
            "Epoch 126/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9412 - accuracy: 0.5736 - val_loss: 0.9696 - val_accuracy: 0.5226\n",
            "Epoch 127/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9376 - accuracy: 0.5944 - val_loss: 0.9691 - val_accuracy: 0.5161\n",
            "Epoch 128/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9380 - accuracy: 0.5806 - val_loss: 0.9693 - val_accuracy: 0.5161\n",
            "Epoch 129/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9350 - accuracy: 0.5958 - val_loss: 0.9681 - val_accuracy: 0.5161\n",
            "Epoch 130/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9449 - accuracy: 0.5667 - val_loss: 0.9674 - val_accuracy: 0.5161\n",
            "Epoch 131/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9273 - accuracy: 0.5944 - val_loss: 0.9657 - val_accuracy: 0.5161\n",
            "Epoch 132/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9307 - accuracy: 0.5764 - val_loss: 0.9646 - val_accuracy: 0.5161\n",
            "Epoch 133/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9341 - accuracy: 0.5764 - val_loss: 0.9633 - val_accuracy: 0.5226\n",
            "Epoch 134/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9347 - accuracy: 0.5708 - val_loss: 0.9623 - val_accuracy: 0.5226\n",
            "Epoch 135/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9320 - accuracy: 0.5500 - val_loss: 0.9614 - val_accuracy: 0.5226\n",
            "Epoch 136/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9235 - accuracy: 0.5722 - val_loss: 0.9602 - val_accuracy: 0.5226\n",
            "Epoch 137/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9318 - accuracy: 0.5764 - val_loss: 0.9596 - val_accuracy: 0.5226\n",
            "Epoch 138/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9337 - accuracy: 0.5722 - val_loss: 0.9586 - val_accuracy: 0.5226\n",
            "Epoch 139/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9240 - accuracy: 0.5778 - val_loss: 0.9582 - val_accuracy: 0.5226\n",
            "Epoch 140/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9278 - accuracy: 0.5764 - val_loss: 0.9574 - val_accuracy: 0.5161\n",
            "Epoch 141/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9257 - accuracy: 0.5708 - val_loss: 0.9557 - val_accuracy: 0.5161\n",
            "Epoch 142/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9304 - accuracy: 0.5681 - val_loss: 0.9546 - val_accuracy: 0.5161\n",
            "Epoch 143/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9296 - accuracy: 0.5694 - val_loss: 0.9539 - val_accuracy: 0.5161\n",
            "Epoch 144/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9291 - accuracy: 0.5736 - val_loss: 0.9529 - val_accuracy: 0.5161\n",
            "Epoch 145/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9209 - accuracy: 0.5778 - val_loss: 0.9517 - val_accuracy: 0.5290\n",
            "Epoch 146/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9186 - accuracy: 0.5778 - val_loss: 0.9496 - val_accuracy: 0.5355\n",
            "Epoch 147/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9136 - accuracy: 0.5847 - val_loss: 0.9481 - val_accuracy: 0.5419\n",
            "Epoch 148/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9199 - accuracy: 0.5736 - val_loss: 0.9472 - val_accuracy: 0.5419\n",
            "Epoch 149/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9280 - accuracy: 0.5750 - val_loss: 0.9465 - val_accuracy: 0.5290\n",
            "Epoch 150/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9121 - accuracy: 0.5764 - val_loss: 0.9455 - val_accuracy: 0.5355\n",
            "Epoch 151/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9209 - accuracy: 0.5778 - val_loss: 0.9449 - val_accuracy: 0.5355\n",
            "Epoch 152/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9217 - accuracy: 0.5833 - val_loss: 0.9436 - val_accuracy: 0.5355\n",
            "Epoch 153/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9108 - accuracy: 0.5847 - val_loss: 0.9430 - val_accuracy: 0.5226\n",
            "Epoch 154/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9121 - accuracy: 0.5889 - val_loss: 0.9419 - val_accuracy: 0.5290\n",
            "Epoch 155/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9137 - accuracy: 0.5917 - val_loss: 0.9408 - val_accuracy: 0.5355\n",
            "Epoch 156/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9102 - accuracy: 0.5931 - val_loss: 0.9399 - val_accuracy: 0.5355\n",
            "Epoch 157/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8989 - accuracy: 0.5917 - val_loss: 0.9385 - val_accuracy: 0.5484\n",
            "Epoch 158/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9159 - accuracy: 0.5694 - val_loss: 0.9380 - val_accuracy: 0.5355\n",
            "Epoch 159/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9160 - accuracy: 0.5764 - val_loss: 0.9375 - val_accuracy: 0.5290\n",
            "Epoch 160/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9110 - accuracy: 0.5792 - val_loss: 0.9368 - val_accuracy: 0.5290\n",
            "Epoch 161/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9050 - accuracy: 0.6056 - val_loss: 0.9357 - val_accuracy: 0.5290\n",
            "Epoch 162/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8884 - accuracy: 0.5903 - val_loss: 0.9343 - val_accuracy: 0.5419\n",
            "Epoch 163/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9020 - accuracy: 0.5750 - val_loss: 0.9332 - val_accuracy: 0.5419\n",
            "Epoch 164/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9005 - accuracy: 0.5764 - val_loss: 0.9324 - val_accuracy: 0.5484\n",
            "Epoch 165/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9043 - accuracy: 0.5806 - val_loss: 0.9313 - val_accuracy: 0.5484\n",
            "Epoch 166/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9100 - accuracy: 0.5750 - val_loss: 0.9306 - val_accuracy: 0.5484\n",
            "Epoch 167/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9026 - accuracy: 0.5792 - val_loss: 0.9298 - val_accuracy: 0.5484\n",
            "Epoch 168/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8842 - accuracy: 0.6069 - val_loss: 0.9296 - val_accuracy: 0.5355\n",
            "Epoch 169/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8990 - accuracy: 0.5861 - val_loss: 0.9300 - val_accuracy: 0.5097\n",
            "Epoch 170/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8935 - accuracy: 0.5917 - val_loss: 0.9290 - val_accuracy: 0.5161\n",
            "Epoch 171/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8951 - accuracy: 0.6000 - val_loss: 0.9283 - val_accuracy: 0.5097\n",
            "Epoch 172/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8978 - accuracy: 0.5806 - val_loss: 0.9269 - val_accuracy: 0.5161\n",
            "Epoch 173/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8959 - accuracy: 0.5833 - val_loss: 0.9258 - val_accuracy: 0.5290\n",
            "Epoch 174/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9038 - accuracy: 0.5847 - val_loss: 0.9247 - val_accuracy: 0.5290\n",
            "Epoch 175/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8877 - accuracy: 0.5889 - val_loss: 0.9230 - val_accuracy: 0.5419\n",
            "Epoch 176/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8881 - accuracy: 0.6097 - val_loss: 0.9228 - val_accuracy: 0.5355\n",
            "Epoch 177/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8990 - accuracy: 0.5750 - val_loss: 0.9227 - val_accuracy: 0.5226\n",
            "Epoch 178/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8835 - accuracy: 0.5764 - val_loss: 0.9223 - val_accuracy: 0.5226\n",
            "Epoch 179/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9071 - accuracy: 0.5667 - val_loss: 0.9211 - val_accuracy: 0.5290\n",
            "Epoch 180/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8863 - accuracy: 0.5972 - val_loss: 0.9205 - val_accuracy: 0.5290\n",
            "Epoch 181/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8886 - accuracy: 0.5847 - val_loss: 0.9192 - val_accuracy: 0.5355\n",
            "Epoch 182/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8886 - accuracy: 0.5931 - val_loss: 0.9192 - val_accuracy: 0.5290\n",
            "Epoch 183/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9009 - accuracy: 0.5931 - val_loss: 0.9183 - val_accuracy: 0.5290\n",
            "Epoch 184/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8922 - accuracy: 0.5889 - val_loss: 0.9171 - val_accuracy: 0.5355\n",
            "Epoch 185/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8873 - accuracy: 0.5986 - val_loss: 0.9162 - val_accuracy: 0.5355\n",
            "Epoch 186/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8878 - accuracy: 0.5694 - val_loss: 0.9153 - val_accuracy: 0.5355\n",
            "Epoch 187/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8817 - accuracy: 0.5944 - val_loss: 0.9135 - val_accuracy: 0.5419\n",
            "Epoch 188/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8821 - accuracy: 0.5972 - val_loss: 0.9130 - val_accuracy: 0.5419\n",
            "Epoch 189/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8741 - accuracy: 0.5944 - val_loss: 0.9119 - val_accuracy: 0.5419\n",
            "Epoch 190/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8820 - accuracy: 0.5958 - val_loss: 0.9110 - val_accuracy: 0.5419\n",
            "Epoch 191/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8917 - accuracy: 0.5833 - val_loss: 0.9113 - val_accuracy: 0.5419\n",
            "Epoch 192/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8806 - accuracy: 0.5958 - val_loss: 0.9104 - val_accuracy: 0.5355\n",
            "Epoch 193/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8902 - accuracy: 0.5917 - val_loss: 0.9099 - val_accuracy: 0.5484\n",
            "Epoch 194/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8763 - accuracy: 0.6056 - val_loss: 0.9091 - val_accuracy: 0.5419\n",
            "Epoch 195/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8876 - accuracy: 0.5847 - val_loss: 0.9077 - val_accuracy: 0.5419\n",
            "Epoch 196/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8922 - accuracy: 0.5889 - val_loss: 0.9079 - val_accuracy: 0.5484\n",
            "Epoch 197/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8749 - accuracy: 0.5792 - val_loss: 0.9069 - val_accuracy: 0.5419\n",
            "Epoch 198/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8821 - accuracy: 0.5889 - val_loss: 0.9063 - val_accuracy: 0.5484\n",
            "Epoch 199/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8786 - accuracy: 0.6056 - val_loss: 0.9063 - val_accuracy: 0.5484\n",
            "Epoch 200/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8721 - accuracy: 0.5917 - val_loss: 0.9051 - val_accuracy: 0.5548\n",
            "Epoch 201/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8848 - accuracy: 0.6042 - val_loss: 0.9045 - val_accuracy: 0.5484\n",
            "Epoch 202/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8659 - accuracy: 0.6167 - val_loss: 0.9033 - val_accuracy: 0.5484\n",
            "Epoch 203/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8774 - accuracy: 0.5944 - val_loss: 0.9037 - val_accuracy: 0.5548\n",
            "Epoch 204/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8812 - accuracy: 0.5792 - val_loss: 0.9021 - val_accuracy: 0.5484\n",
            "Epoch 205/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8589 - accuracy: 0.6014 - val_loss: 0.9011 - val_accuracy: 0.5484\n",
            "Epoch 206/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8651 - accuracy: 0.6014 - val_loss: 0.9001 - val_accuracy: 0.5548\n",
            "Epoch 207/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8786 - accuracy: 0.5764 - val_loss: 0.9005 - val_accuracy: 0.5548\n",
            "Epoch 208/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8672 - accuracy: 0.6097 - val_loss: 0.8998 - val_accuracy: 0.5548\n",
            "Epoch 209/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8759 - accuracy: 0.5764 - val_loss: 0.8985 - val_accuracy: 0.5548\n",
            "Epoch 210/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8678 - accuracy: 0.6056 - val_loss: 0.8992 - val_accuracy: 0.5548\n",
            "Epoch 211/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8691 - accuracy: 0.5861 - val_loss: 0.8976 - val_accuracy: 0.5484\n",
            "Epoch 212/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8650 - accuracy: 0.5861 - val_loss: 0.8962 - val_accuracy: 0.5548\n",
            "Epoch 213/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8636 - accuracy: 0.5944 - val_loss: 0.8952 - val_accuracy: 0.5548\n",
            "Epoch 214/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8634 - accuracy: 0.5889 - val_loss: 0.8942 - val_accuracy: 0.5613\n",
            "Epoch 215/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8655 - accuracy: 0.5889 - val_loss: 0.8933 - val_accuracy: 0.5613\n",
            "Epoch 216/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8633 - accuracy: 0.5861 - val_loss: 0.8927 - val_accuracy: 0.5613\n",
            "Epoch 217/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8625 - accuracy: 0.5944 - val_loss: 0.8924 - val_accuracy: 0.5548\n",
            "Epoch 218/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8544 - accuracy: 0.6139 - val_loss: 0.8912 - val_accuracy: 0.5548\n",
            "Epoch 219/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8667 - accuracy: 0.5903 - val_loss: 0.8910 - val_accuracy: 0.5548\n",
            "Epoch 220/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8658 - accuracy: 0.5903 - val_loss: 0.8894 - val_accuracy: 0.5677\n",
            "Epoch 221/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8667 - accuracy: 0.5931 - val_loss: 0.8886 - val_accuracy: 0.5677\n",
            "Epoch 222/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8614 - accuracy: 0.6139 - val_loss: 0.8884 - val_accuracy: 0.5613\n",
            "Epoch 223/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8613 - accuracy: 0.6014 - val_loss: 0.8874 - val_accuracy: 0.5677\n",
            "Epoch 224/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8670 - accuracy: 0.5944 - val_loss: 0.8862 - val_accuracy: 0.5742\n",
            "Epoch 225/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8601 - accuracy: 0.5847 - val_loss: 0.8853 - val_accuracy: 0.5742\n",
            "Epoch 226/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8427 - accuracy: 0.6139 - val_loss: 0.8843 - val_accuracy: 0.5742\n",
            "Epoch 227/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8630 - accuracy: 0.5875 - val_loss: 0.8849 - val_accuracy: 0.5677\n",
            "Epoch 228/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8556 - accuracy: 0.6069 - val_loss: 0.8842 - val_accuracy: 0.5677\n",
            "Epoch 229/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8514 - accuracy: 0.6097 - val_loss: 0.8834 - val_accuracy: 0.5677\n",
            "Epoch 230/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8551 - accuracy: 0.5986 - val_loss: 0.8835 - val_accuracy: 0.5613\n",
            "Epoch 231/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8473 - accuracy: 0.6208 - val_loss: 0.8836 - val_accuracy: 0.5613\n",
            "Epoch 232/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8630 - accuracy: 0.6056 - val_loss: 0.8828 - val_accuracy: 0.5613\n",
            "Epoch 233/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8543 - accuracy: 0.6028 - val_loss: 0.8826 - val_accuracy: 0.5613\n",
            "Epoch 234/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8638 - accuracy: 0.5931 - val_loss: 0.8826 - val_accuracy: 0.5484\n",
            "Epoch 235/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8576 - accuracy: 0.6083 - val_loss: 0.8811 - val_accuracy: 0.5613\n",
            "Epoch 236/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8561 - accuracy: 0.6222 - val_loss: 0.8807 - val_accuracy: 0.5613\n",
            "Epoch 237/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8570 - accuracy: 0.6042 - val_loss: 0.8792 - val_accuracy: 0.5613\n",
            "Epoch 238/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8464 - accuracy: 0.6056 - val_loss: 0.8792 - val_accuracy: 0.5613\n",
            "Epoch 239/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8381 - accuracy: 0.6069 - val_loss: 0.8790 - val_accuracy: 0.5613\n",
            "Epoch 240/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8547 - accuracy: 0.6014 - val_loss: 0.8784 - val_accuracy: 0.5548\n",
            "Epoch 241/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8495 - accuracy: 0.6028 - val_loss: 0.8772 - val_accuracy: 0.5613\n",
            "Epoch 242/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8539 - accuracy: 0.5972 - val_loss: 0.8761 - val_accuracy: 0.5613\n",
            "Epoch 243/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8577 - accuracy: 0.5903 - val_loss: 0.8753 - val_accuracy: 0.5613\n",
            "Epoch 244/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8491 - accuracy: 0.6139 - val_loss: 0.8752 - val_accuracy: 0.5613\n",
            "Epoch 245/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8356 - accuracy: 0.6056 - val_loss: 0.8736 - val_accuracy: 0.5677\n",
            "Epoch 246/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8524 - accuracy: 0.6097 - val_loss: 0.8737 - val_accuracy: 0.5613\n",
            "Epoch 247/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8377 - accuracy: 0.6236 - val_loss: 0.8731 - val_accuracy: 0.5613\n",
            "Epoch 248/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8406 - accuracy: 0.5875 - val_loss: 0.8722 - val_accuracy: 0.5677\n",
            "Epoch 249/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8360 - accuracy: 0.6069 - val_loss: 0.8711 - val_accuracy: 0.5742\n",
            "Epoch 250/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8489 - accuracy: 0.6083 - val_loss: 0.8709 - val_accuracy: 0.5677\n",
            "Epoch 251/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8448 - accuracy: 0.6056 - val_loss: 0.8700 - val_accuracy: 0.5742\n",
            "Epoch 252/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8374 - accuracy: 0.6208 - val_loss: 0.8702 - val_accuracy: 0.5613\n",
            "Epoch 253/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8424 - accuracy: 0.6028 - val_loss: 0.8704 - val_accuracy: 0.5613\n",
            "Epoch 254/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8325 - accuracy: 0.6028 - val_loss: 0.8692 - val_accuracy: 0.5613\n",
            "Epoch 255/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8344 - accuracy: 0.6222 - val_loss: 0.8682 - val_accuracy: 0.5677\n",
            "Epoch 256/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8417 - accuracy: 0.6194 - val_loss: 0.8667 - val_accuracy: 0.5742\n",
            "Epoch 257/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8405 - accuracy: 0.6111 - val_loss: 0.8660 - val_accuracy: 0.5806\n",
            "Epoch 258/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8298 - accuracy: 0.6153 - val_loss: 0.8652 - val_accuracy: 0.5806\n",
            "Epoch 259/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8375 - accuracy: 0.6139 - val_loss: 0.8649 - val_accuracy: 0.5806\n",
            "Epoch 260/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8435 - accuracy: 0.5958 - val_loss: 0.8652 - val_accuracy: 0.5742\n",
            "Epoch 261/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8499 - accuracy: 0.6042 - val_loss: 0.8648 - val_accuracy: 0.5742\n",
            "Epoch 262/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8332 - accuracy: 0.5972 - val_loss: 0.8637 - val_accuracy: 0.5742\n",
            "Epoch 263/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8503 - accuracy: 0.6056 - val_loss: 0.8627 - val_accuracy: 0.5806\n",
            "Epoch 264/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8430 - accuracy: 0.6014 - val_loss: 0.8623 - val_accuracy: 0.5806\n",
            "Epoch 265/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8338 - accuracy: 0.6097 - val_loss: 0.8613 - val_accuracy: 0.5871\n",
            "Epoch 266/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8415 - accuracy: 0.6194 - val_loss: 0.8605 - val_accuracy: 0.5806\n",
            "Epoch 267/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8351 - accuracy: 0.6056 - val_loss: 0.8609 - val_accuracy: 0.5806\n",
            "Epoch 268/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8242 - accuracy: 0.6042 - val_loss: 0.8595 - val_accuracy: 0.5806\n",
            "Epoch 269/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8313 - accuracy: 0.6028 - val_loss: 0.8594 - val_accuracy: 0.5742\n",
            "Epoch 270/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8170 - accuracy: 0.6333 - val_loss: 0.8587 - val_accuracy: 0.5742\n",
            "Epoch 271/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8366 - accuracy: 0.6069 - val_loss: 0.8584 - val_accuracy: 0.5806\n",
            "Epoch 272/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8173 - accuracy: 0.6139 - val_loss: 0.8585 - val_accuracy: 0.5742\n",
            "Epoch 273/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8309 - accuracy: 0.6194 - val_loss: 0.8585 - val_accuracy: 0.5742\n",
            "Epoch 274/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8268 - accuracy: 0.6042 - val_loss: 0.8578 - val_accuracy: 0.5742\n",
            "Epoch 275/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8301 - accuracy: 0.6153 - val_loss: 0.8569 - val_accuracy: 0.5742\n",
            "Epoch 276/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8333 - accuracy: 0.6097 - val_loss: 0.8548 - val_accuracy: 0.5871\n",
            "Epoch 277/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8263 - accuracy: 0.6319 - val_loss: 0.8551 - val_accuracy: 0.5742\n",
            "Epoch 278/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8259 - accuracy: 0.6083 - val_loss: 0.8553 - val_accuracy: 0.5742\n",
            "Epoch 279/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8282 - accuracy: 0.6153 - val_loss: 0.8545 - val_accuracy: 0.5742\n",
            "Epoch 280/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8127 - accuracy: 0.6306 - val_loss: 0.8537 - val_accuracy: 0.5806\n",
            "Epoch 281/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8262 - accuracy: 0.6000 - val_loss: 0.8525 - val_accuracy: 0.5871\n",
            "Epoch 282/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8249 - accuracy: 0.6167 - val_loss: 0.8529 - val_accuracy: 0.5742\n",
            "Epoch 283/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8341 - accuracy: 0.5944 - val_loss: 0.8526 - val_accuracy: 0.5742\n",
            "Epoch 284/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8255 - accuracy: 0.6097 - val_loss: 0.8515 - val_accuracy: 0.5806\n",
            "Epoch 285/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8231 - accuracy: 0.6028 - val_loss: 0.8511 - val_accuracy: 0.5742\n",
            "Epoch 286/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8213 - accuracy: 0.6167 - val_loss: 0.8509 - val_accuracy: 0.5742\n",
            "Epoch 287/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8195 - accuracy: 0.6222 - val_loss: 0.8514 - val_accuracy: 0.5742\n",
            "Epoch 288/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8234 - accuracy: 0.6069 - val_loss: 0.8500 - val_accuracy: 0.5742\n",
            "Epoch 289/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8248 - accuracy: 0.6236 - val_loss: 0.8501 - val_accuracy: 0.5742\n",
            "Epoch 290/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8260 - accuracy: 0.6139 - val_loss: 0.8498 - val_accuracy: 0.5742\n",
            "Epoch 291/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8222 - accuracy: 0.6167 - val_loss: 0.8478 - val_accuracy: 0.5871\n",
            "Epoch 292/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8304 - accuracy: 0.6139 - val_loss: 0.8471 - val_accuracy: 0.5871\n",
            "Epoch 293/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8278 - accuracy: 0.6069 - val_loss: 0.8460 - val_accuracy: 0.5871\n",
            "Epoch 294/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8141 - accuracy: 0.6111 - val_loss: 0.8457 - val_accuracy: 0.5871\n",
            "Epoch 295/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8293 - accuracy: 0.5958 - val_loss: 0.8447 - val_accuracy: 0.5871\n",
            "Epoch 296/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8152 - accuracy: 0.6361 - val_loss: 0.8439 - val_accuracy: 0.5871\n",
            "Epoch 297/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8107 - accuracy: 0.6361 - val_loss: 0.8435 - val_accuracy: 0.5935\n",
            "Epoch 298/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8308 - accuracy: 0.5875 - val_loss: 0.8432 - val_accuracy: 0.5935\n",
            "Epoch 299/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8140 - accuracy: 0.6403 - val_loss: 0.8430 - val_accuracy: 0.5935\n",
            "Epoch 300/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8115 - accuracy: 0.6222 - val_loss: 0.8422 - val_accuracy: 0.5935\n",
            "Epoch 301/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8119 - accuracy: 0.6069 - val_loss: 0.8415 - val_accuracy: 0.5871\n",
            "Epoch 302/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8059 - accuracy: 0.6194 - val_loss: 0.8424 - val_accuracy: 0.5871\n",
            "Epoch 303/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8051 - accuracy: 0.6236 - val_loss: 0.8417 - val_accuracy: 0.5871\n",
            "Epoch 304/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7975 - accuracy: 0.6208 - val_loss: 0.8415 - val_accuracy: 0.5871\n",
            "Epoch 305/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8146 - accuracy: 0.6083 - val_loss: 0.8407 - val_accuracy: 0.5935\n",
            "Epoch 306/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8204 - accuracy: 0.6222 - val_loss: 0.8399 - val_accuracy: 0.5935\n",
            "Epoch 307/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8179 - accuracy: 0.6222 - val_loss: 0.8400 - val_accuracy: 0.5871\n",
            "Epoch 308/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8222 - accuracy: 0.5944 - val_loss: 0.8391 - val_accuracy: 0.5935\n",
            "Epoch 309/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8023 - accuracy: 0.6389 - val_loss: 0.8394 - val_accuracy: 0.5871\n",
            "Epoch 310/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8015 - accuracy: 0.6431 - val_loss: 0.8373 - val_accuracy: 0.5935\n",
            "Epoch 311/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8272 - accuracy: 0.6042 - val_loss: 0.8367 - val_accuracy: 0.5935\n",
            "Epoch 312/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8108 - accuracy: 0.6306 - val_loss: 0.8364 - val_accuracy: 0.5935\n",
            "Epoch 313/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8119 - accuracy: 0.6250 - val_loss: 0.8361 - val_accuracy: 0.5935\n",
            "Epoch 314/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8188 - accuracy: 0.6153 - val_loss: 0.8362 - val_accuracy: 0.5935\n",
            "Epoch 315/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8081 - accuracy: 0.6167 - val_loss: 0.8365 - val_accuracy: 0.5935\n",
            "Epoch 316/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8083 - accuracy: 0.6319 - val_loss: 0.8369 - val_accuracy: 0.5871\n",
            "Epoch 317/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8123 - accuracy: 0.6333 - val_loss: 0.8362 - val_accuracy: 0.5871\n",
            "Epoch 318/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8139 - accuracy: 0.6208 - val_loss: 0.8358 - val_accuracy: 0.5935\n",
            "Epoch 319/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8077 - accuracy: 0.6194 - val_loss: 0.8344 - val_accuracy: 0.5935\n",
            "Epoch 320/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8175 - accuracy: 0.6375 - val_loss: 0.8349 - val_accuracy: 0.5871\n",
            "Epoch 321/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8125 - accuracy: 0.6306 - val_loss: 0.8342 - val_accuracy: 0.5935\n",
            "Epoch 322/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8040 - accuracy: 0.6264 - val_loss: 0.8335 - val_accuracy: 0.5935\n",
            "Epoch 323/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7985 - accuracy: 0.6347 - val_loss: 0.8330 - val_accuracy: 0.5935\n",
            "Epoch 324/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7962 - accuracy: 0.6181 - val_loss: 0.8321 - val_accuracy: 0.5935\n",
            "Epoch 325/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8014 - accuracy: 0.6319 - val_loss: 0.8312 - val_accuracy: 0.5935\n",
            "Epoch 326/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8151 - accuracy: 0.6042 - val_loss: 0.8310 - val_accuracy: 0.5935\n",
            "Epoch 327/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7954 - accuracy: 0.6264 - val_loss: 0.8304 - val_accuracy: 0.5935\n",
            "Epoch 328/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8047 - accuracy: 0.6361 - val_loss: 0.8297 - val_accuracy: 0.5935\n",
            "Epoch 329/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8125 - accuracy: 0.6139 - val_loss: 0.8293 - val_accuracy: 0.6000\n",
            "Epoch 330/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8052 - accuracy: 0.6333 - val_loss: 0.8287 - val_accuracy: 0.6000\n",
            "Epoch 331/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8184 - accuracy: 0.6028 - val_loss: 0.8279 - val_accuracy: 0.6000\n",
            "Epoch 332/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7867 - accuracy: 0.6500 - val_loss: 0.8274 - val_accuracy: 0.6000\n",
            "Epoch 333/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7960 - accuracy: 0.6306 - val_loss: 0.8278 - val_accuracy: 0.5935\n",
            "Epoch 334/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8038 - accuracy: 0.6167 - val_loss: 0.8263 - val_accuracy: 0.6129\n",
            "Epoch 335/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8131 - accuracy: 0.6125 - val_loss: 0.8260 - val_accuracy: 0.6065\n",
            "Epoch 336/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8042 - accuracy: 0.6167 - val_loss: 0.8260 - val_accuracy: 0.6000\n",
            "Epoch 337/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8111 - accuracy: 0.6458 - val_loss: 0.8257 - val_accuracy: 0.6000\n",
            "Epoch 338/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8012 - accuracy: 0.6236 - val_loss: 0.8262 - val_accuracy: 0.6000\n",
            "Epoch 339/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7981 - accuracy: 0.6083 - val_loss: 0.8255 - val_accuracy: 0.6000\n",
            "Epoch 340/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7895 - accuracy: 0.6431 - val_loss: 0.8252 - val_accuracy: 0.5935\n",
            "Epoch 341/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7988 - accuracy: 0.6347 - val_loss: 0.8241 - val_accuracy: 0.6065\n",
            "Epoch 342/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8071 - accuracy: 0.6167 - val_loss: 0.8236 - val_accuracy: 0.6065\n",
            "Epoch 343/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7952 - accuracy: 0.6208 - val_loss: 0.8227 - val_accuracy: 0.6065\n",
            "Epoch 344/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7999 - accuracy: 0.6264 - val_loss: 0.8233 - val_accuracy: 0.5935\n",
            "Epoch 345/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7967 - accuracy: 0.6417 - val_loss: 0.8219 - val_accuracy: 0.6065\n",
            "Epoch 346/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7974 - accuracy: 0.6278 - val_loss: 0.8228 - val_accuracy: 0.6000\n",
            "Epoch 347/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8050 - accuracy: 0.6167 - val_loss: 0.8223 - val_accuracy: 0.5935\n",
            "Epoch 348/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8090 - accuracy: 0.6125 - val_loss: 0.8217 - val_accuracy: 0.6000\n",
            "Epoch 349/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7953 - accuracy: 0.6278 - val_loss: 0.8203 - val_accuracy: 0.6065\n",
            "Epoch 350/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8037 - accuracy: 0.6139 - val_loss: 0.8205 - val_accuracy: 0.6065\n",
            "Epoch 351/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8057 - accuracy: 0.6306 - val_loss: 0.8203 - val_accuracy: 0.6000\n",
            "Epoch 352/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7896 - accuracy: 0.6361 - val_loss: 0.8202 - val_accuracy: 0.6000\n",
            "Epoch 353/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7918 - accuracy: 0.6333 - val_loss: 0.8186 - val_accuracy: 0.6065\n",
            "Epoch 354/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7931 - accuracy: 0.6375 - val_loss: 0.8190 - val_accuracy: 0.6065\n",
            "Epoch 355/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7907 - accuracy: 0.6375 - val_loss: 0.8179 - val_accuracy: 0.6065\n",
            "Epoch 356/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8086 - accuracy: 0.6208 - val_loss: 0.8194 - val_accuracy: 0.6000\n",
            "Epoch 357/1000\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.8009 - accuracy: 0.6139 - val_loss: 0.8183 - val_accuracy: 0.6000\n",
            "Epoch 358/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8001 - accuracy: 0.6264 - val_loss: 0.8182 - val_accuracy: 0.6000\n",
            "Epoch 359/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7855 - accuracy: 0.6306 - val_loss: 0.8172 - val_accuracy: 0.6065\n",
            "Epoch 360/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8003 - accuracy: 0.6458 - val_loss: 0.8173 - val_accuracy: 0.6000\n",
            "Epoch 361/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7842 - accuracy: 0.6250 - val_loss: 0.8170 - val_accuracy: 0.6000\n",
            "Epoch 362/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7902 - accuracy: 0.6222 - val_loss: 0.8155 - val_accuracy: 0.6065\n",
            "Epoch 363/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7939 - accuracy: 0.6389 - val_loss: 0.8158 - val_accuracy: 0.6000\n",
            "Epoch 364/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8000 - accuracy: 0.6208 - val_loss: 0.8154 - val_accuracy: 0.6000\n",
            "Epoch 365/1000\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.7912 - accuracy: 0.6431 - val_loss: 0.8147 - val_accuracy: 0.6065\n",
            "Epoch 366/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7978 - accuracy: 0.6181 - val_loss: 0.8144 - val_accuracy: 0.6065\n",
            "Epoch 367/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7842 - accuracy: 0.6292 - val_loss: 0.8141 - val_accuracy: 0.6065\n",
            "Epoch 368/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7886 - accuracy: 0.6444 - val_loss: 0.8126 - val_accuracy: 0.6129\n",
            "Epoch 369/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7834 - accuracy: 0.6389 - val_loss: 0.8117 - val_accuracy: 0.6129\n",
            "Epoch 370/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7839 - accuracy: 0.6306 - val_loss: 0.8126 - val_accuracy: 0.6065\n",
            "Epoch 371/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7862 - accuracy: 0.6264 - val_loss: 0.8135 - val_accuracy: 0.6000\n",
            "Epoch 372/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7887 - accuracy: 0.6389 - val_loss: 0.8127 - val_accuracy: 0.6000\n",
            "Epoch 373/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7996 - accuracy: 0.6208 - val_loss: 0.8110 - val_accuracy: 0.6129\n",
            "Epoch 374/1000\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7951 - accuracy: 0.6278 - val_loss: 0.8112 - val_accuracy: 0.6065\n",
            "Epoch 375/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7825 - accuracy: 0.6361 - val_loss: 0.8106 - val_accuracy: 0.6065\n",
            "Epoch 376/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7991 - accuracy: 0.6167 - val_loss: 0.8101 - val_accuracy: 0.6065\n",
            "Epoch 377/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7783 - accuracy: 0.6194 - val_loss: 0.8106 - val_accuracy: 0.6065\n",
            "Epoch 378/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7876 - accuracy: 0.6292 - val_loss: 0.8102 - val_accuracy: 0.6065\n",
            "Epoch 379/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7991 - accuracy: 0.6306 - val_loss: 0.8085 - val_accuracy: 0.6129\n",
            "Epoch 380/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7800 - accuracy: 0.6403 - val_loss: 0.8085 - val_accuracy: 0.6129\n",
            "Epoch 381/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7725 - accuracy: 0.6417 - val_loss: 0.8085 - val_accuracy: 0.6129\n",
            "Epoch 382/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7821 - accuracy: 0.6458 - val_loss: 0.8082 - val_accuracy: 0.6129\n",
            "Epoch 383/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7913 - accuracy: 0.6194 - val_loss: 0.8088 - val_accuracy: 0.6000\n",
            "Epoch 384/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7849 - accuracy: 0.6222 - val_loss: 0.8070 - val_accuracy: 0.6129\n",
            "Epoch 385/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7876 - accuracy: 0.6264 - val_loss: 0.8074 - val_accuracy: 0.6129\n",
            "Epoch 386/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7863 - accuracy: 0.6403 - val_loss: 0.8070 - val_accuracy: 0.6129\n",
            "Epoch 387/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7922 - accuracy: 0.6361 - val_loss: 0.8077 - val_accuracy: 0.6000\n",
            "Epoch 388/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7852 - accuracy: 0.6194 - val_loss: 0.8066 - val_accuracy: 0.6129\n",
            "Epoch 389/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7930 - accuracy: 0.6181 - val_loss: 0.8059 - val_accuracy: 0.6129\n",
            "Epoch 390/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7829 - accuracy: 0.6597 - val_loss: 0.8050 - val_accuracy: 0.6129\n",
            "Epoch 391/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7901 - accuracy: 0.6375 - val_loss: 0.8043 - val_accuracy: 0.6129\n",
            "Epoch 392/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7830 - accuracy: 0.6444 - val_loss: 0.8050 - val_accuracy: 0.6129\n",
            "Epoch 393/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7799 - accuracy: 0.6208 - val_loss: 0.8051 - val_accuracy: 0.6129\n",
            "Epoch 394/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7838 - accuracy: 0.6431 - val_loss: 0.8045 - val_accuracy: 0.6129\n",
            "Epoch 395/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7773 - accuracy: 0.6306 - val_loss: 0.8041 - val_accuracy: 0.6129\n",
            "Epoch 396/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7835 - accuracy: 0.6264 - val_loss: 0.8039 - val_accuracy: 0.6129\n",
            "Epoch 397/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7788 - accuracy: 0.6403 - val_loss: 0.8036 - val_accuracy: 0.6129\n",
            "Epoch 398/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7739 - accuracy: 0.6431 - val_loss: 0.8036 - val_accuracy: 0.6129\n",
            "Epoch 399/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7850 - accuracy: 0.6500 - val_loss: 0.8044 - val_accuracy: 0.6129\n",
            "Epoch 400/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7814 - accuracy: 0.6278 - val_loss: 0.8041 - val_accuracy: 0.6129\n",
            "Epoch 401/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7858 - accuracy: 0.6194 - val_loss: 0.8026 - val_accuracy: 0.6129\n",
            "Epoch 402/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7796 - accuracy: 0.6347 - val_loss: 0.8027 - val_accuracy: 0.6129\n",
            "Epoch 403/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7767 - accuracy: 0.6444 - val_loss: 0.8026 - val_accuracy: 0.6129\n",
            "Epoch 404/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7653 - accuracy: 0.6528 - val_loss: 0.8020 - val_accuracy: 0.6129\n",
            "Epoch 405/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7735 - accuracy: 0.6292 - val_loss: 0.8015 - val_accuracy: 0.6129\n",
            "Epoch 406/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7756 - accuracy: 0.6514 - val_loss: 0.8018 - val_accuracy: 0.6194\n",
            "Epoch 407/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7713 - accuracy: 0.6417 - val_loss: 0.8025 - val_accuracy: 0.6065\n",
            "Epoch 408/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7858 - accuracy: 0.6236 - val_loss: 0.8009 - val_accuracy: 0.6194\n",
            "Epoch 409/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7872 - accuracy: 0.6181 - val_loss: 0.7995 - val_accuracy: 0.6129\n",
            "Epoch 410/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7779 - accuracy: 0.6181 - val_loss: 0.7993 - val_accuracy: 0.6129\n",
            "Epoch 411/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7745 - accuracy: 0.6569 - val_loss: 0.7999 - val_accuracy: 0.6194\n",
            "Epoch 412/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7768 - accuracy: 0.6375 - val_loss: 0.7997 - val_accuracy: 0.6194\n",
            "Epoch 413/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7829 - accuracy: 0.6194 - val_loss: 0.7984 - val_accuracy: 0.6129\n",
            "Epoch 414/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7801 - accuracy: 0.6278 - val_loss: 0.7973 - val_accuracy: 0.6194\n",
            "Epoch 415/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7829 - accuracy: 0.6319 - val_loss: 0.7976 - val_accuracy: 0.6129\n",
            "Epoch 416/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7763 - accuracy: 0.6514 - val_loss: 0.7971 - val_accuracy: 0.6129\n",
            "Epoch 417/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7703 - accuracy: 0.6431 - val_loss: 0.7973 - val_accuracy: 0.6194\n",
            "Epoch 418/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7700 - accuracy: 0.6417 - val_loss: 0.7979 - val_accuracy: 0.6258\n",
            "Epoch 419/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7721 - accuracy: 0.6361 - val_loss: 0.7963 - val_accuracy: 0.6194\n",
            "Epoch 420/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7698 - accuracy: 0.6208 - val_loss: 0.7959 - val_accuracy: 0.6194\n",
            "Epoch 421/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7691 - accuracy: 0.6333 - val_loss: 0.7954 - val_accuracy: 0.6194\n",
            "Epoch 422/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7722 - accuracy: 0.6486 - val_loss: 0.7968 - val_accuracy: 0.6258\n",
            "Epoch 423/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7718 - accuracy: 0.6417 - val_loss: 0.7969 - val_accuracy: 0.6194\n",
            "Epoch 424/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7810 - accuracy: 0.6333 - val_loss: 0.7960 - val_accuracy: 0.6258\n",
            "Epoch 425/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7714 - accuracy: 0.6500 - val_loss: 0.7957 - val_accuracy: 0.6258\n",
            "Epoch 426/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7827 - accuracy: 0.6194 - val_loss: 0.7941 - val_accuracy: 0.6194\n",
            "Epoch 427/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7736 - accuracy: 0.6458 - val_loss: 0.7934 - val_accuracy: 0.6129\n",
            "Epoch 428/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7760 - accuracy: 0.6306 - val_loss: 0.7936 - val_accuracy: 0.6194\n",
            "Epoch 429/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7673 - accuracy: 0.6444 - val_loss: 0.7920 - val_accuracy: 0.6129\n",
            "Epoch 430/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7782 - accuracy: 0.6306 - val_loss: 0.7922 - val_accuracy: 0.6194\n",
            "Epoch 431/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7671 - accuracy: 0.6278 - val_loss: 0.7919 - val_accuracy: 0.6194\n",
            "Epoch 432/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7702 - accuracy: 0.6389 - val_loss: 0.7914 - val_accuracy: 0.6129\n",
            "Epoch 433/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7807 - accuracy: 0.6403 - val_loss: 0.7911 - val_accuracy: 0.6194\n",
            "Epoch 434/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7701 - accuracy: 0.6389 - val_loss: 0.7917 - val_accuracy: 0.6258\n",
            "Epoch 435/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7654 - accuracy: 0.6472 - val_loss: 0.7920 - val_accuracy: 0.6323\n",
            "Epoch 436/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7626 - accuracy: 0.6653 - val_loss: 0.7905 - val_accuracy: 0.6258\n",
            "Epoch 437/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7827 - accuracy: 0.6403 - val_loss: 0.7902 - val_accuracy: 0.6258\n",
            "Epoch 438/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7581 - accuracy: 0.6292 - val_loss: 0.7907 - val_accuracy: 0.6258\n",
            "Epoch 439/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7719 - accuracy: 0.6264 - val_loss: 0.7902 - val_accuracy: 0.6258\n",
            "Epoch 440/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7624 - accuracy: 0.6556 - val_loss: 0.7892 - val_accuracy: 0.6194\n",
            "Epoch 441/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7804 - accuracy: 0.6361 - val_loss: 0.7895 - val_accuracy: 0.6258\n",
            "Epoch 442/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7703 - accuracy: 0.6347 - val_loss: 0.7888 - val_accuracy: 0.6258\n",
            "Epoch 443/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7590 - accuracy: 0.6458 - val_loss: 0.7890 - val_accuracy: 0.6258\n",
            "Epoch 444/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7799 - accuracy: 0.6292 - val_loss: 0.7885 - val_accuracy: 0.6258\n",
            "Epoch 445/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7716 - accuracy: 0.6403 - val_loss: 0.7885 - val_accuracy: 0.6258\n",
            "Epoch 446/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7643 - accuracy: 0.6417 - val_loss: 0.7885 - val_accuracy: 0.6323\n",
            "Epoch 447/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7723 - accuracy: 0.6403 - val_loss: 0.7879 - val_accuracy: 0.6258\n",
            "Epoch 448/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7727 - accuracy: 0.6431 - val_loss: 0.7880 - val_accuracy: 0.6258\n",
            "Epoch 449/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7637 - accuracy: 0.6486 - val_loss: 0.7880 - val_accuracy: 0.6258\n",
            "Epoch 450/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7800 - accuracy: 0.6403 - val_loss: 0.7875 - val_accuracy: 0.6258\n",
            "Epoch 451/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7656 - accuracy: 0.6306 - val_loss: 0.7868 - val_accuracy: 0.6258\n",
            "Epoch 452/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7550 - accuracy: 0.6375 - val_loss: 0.7876 - val_accuracy: 0.6323\n",
            "Epoch 453/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7554 - accuracy: 0.6306 - val_loss: 0.7869 - val_accuracy: 0.6323\n",
            "Epoch 454/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7610 - accuracy: 0.6431 - val_loss: 0.7867 - val_accuracy: 0.6323\n",
            "Epoch 455/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7541 - accuracy: 0.6542 - val_loss: 0.7854 - val_accuracy: 0.6323\n",
            "Epoch 456/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7661 - accuracy: 0.6222 - val_loss: 0.7858 - val_accuracy: 0.6258\n",
            "Epoch 457/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7575 - accuracy: 0.6431 - val_loss: 0.7850 - val_accuracy: 0.6387\n",
            "Epoch 458/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7613 - accuracy: 0.6486 - val_loss: 0.7848 - val_accuracy: 0.6323\n",
            "Epoch 459/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7561 - accuracy: 0.6403 - val_loss: 0.7855 - val_accuracy: 0.6323\n",
            "Epoch 460/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7717 - accuracy: 0.6500 - val_loss: 0.7860 - val_accuracy: 0.6323\n",
            "Epoch 461/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7687 - accuracy: 0.6375 - val_loss: 0.7865 - val_accuracy: 0.6323\n",
            "Epoch 462/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7618 - accuracy: 0.6361 - val_loss: 0.7846 - val_accuracy: 0.6452\n",
            "Epoch 463/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7807 - accuracy: 0.6250 - val_loss: 0.7837 - val_accuracy: 0.6452\n",
            "Epoch 464/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7686 - accuracy: 0.6375 - val_loss: 0.7849 - val_accuracy: 0.6387\n",
            "Epoch 465/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7581 - accuracy: 0.6556 - val_loss: 0.7835 - val_accuracy: 0.6516\n",
            "Epoch 466/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7603 - accuracy: 0.6486 - val_loss: 0.7832 - val_accuracy: 0.6452\n",
            "Epoch 467/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7566 - accuracy: 0.6306 - val_loss: 0.7841 - val_accuracy: 0.6258\n",
            "Epoch 468/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7607 - accuracy: 0.6306 - val_loss: 0.7831 - val_accuracy: 0.6516\n",
            "Epoch 469/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7747 - accuracy: 0.6361 - val_loss: 0.7815 - val_accuracy: 0.6387\n",
            "Epoch 470/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7669 - accuracy: 0.6431 - val_loss: 0.7814 - val_accuracy: 0.6323\n",
            "Epoch 471/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7664 - accuracy: 0.6389 - val_loss: 0.7816 - val_accuracy: 0.6452\n",
            "Epoch 472/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7677 - accuracy: 0.6361 - val_loss: 0.7831 - val_accuracy: 0.6387\n",
            "Epoch 473/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7648 - accuracy: 0.6458 - val_loss: 0.7831 - val_accuracy: 0.6387\n",
            "Epoch 474/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7498 - accuracy: 0.6528 - val_loss: 0.7822 - val_accuracy: 0.6516\n",
            "Epoch 475/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7549 - accuracy: 0.6681 - val_loss: 0.7820 - val_accuracy: 0.6516\n",
            "Epoch 476/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7640 - accuracy: 0.6333 - val_loss: 0.7789 - val_accuracy: 0.6516\n",
            "Epoch 477/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7497 - accuracy: 0.6500 - val_loss: 0.7784 - val_accuracy: 0.6516\n",
            "Epoch 478/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7615 - accuracy: 0.6500 - val_loss: 0.7792 - val_accuracy: 0.6452\n",
            "Epoch 479/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7611 - accuracy: 0.6528 - val_loss: 0.7792 - val_accuracy: 0.6452\n",
            "Epoch 480/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7651 - accuracy: 0.6458 - val_loss: 0.7794 - val_accuracy: 0.6387\n",
            "Epoch 481/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7655 - accuracy: 0.6375 - val_loss: 0.7792 - val_accuracy: 0.6387\n",
            "Epoch 482/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7572 - accuracy: 0.6486 - val_loss: 0.7787 - val_accuracy: 0.6387\n",
            "Epoch 483/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7599 - accuracy: 0.6319 - val_loss: 0.7790 - val_accuracy: 0.6387\n",
            "Epoch 484/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7650 - accuracy: 0.6403 - val_loss: 0.7793 - val_accuracy: 0.6516\n",
            "Epoch 485/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7654 - accuracy: 0.6403 - val_loss: 0.7794 - val_accuracy: 0.6516\n",
            "Epoch 486/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7645 - accuracy: 0.6222 - val_loss: 0.7779 - val_accuracy: 0.6452\n",
            "Epoch 487/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7497 - accuracy: 0.6542 - val_loss: 0.7777 - val_accuracy: 0.6387\n",
            "Epoch 488/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7417 - accuracy: 0.6556 - val_loss: 0.7771 - val_accuracy: 0.6452\n",
            "Epoch 489/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7672 - accuracy: 0.6278 - val_loss: 0.7762 - val_accuracy: 0.6516\n",
            "Epoch 490/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7720 - accuracy: 0.6458 - val_loss: 0.7757 - val_accuracy: 0.6581\n",
            "Epoch 491/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7621 - accuracy: 0.6472 - val_loss: 0.7762 - val_accuracy: 0.6516\n",
            "Epoch 492/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7633 - accuracy: 0.6444 - val_loss: 0.7755 - val_accuracy: 0.6645\n",
            "Epoch 493/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7664 - accuracy: 0.6333 - val_loss: 0.7749 - val_accuracy: 0.6645\n",
            "Epoch 494/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7613 - accuracy: 0.6417 - val_loss: 0.7761 - val_accuracy: 0.6452\n",
            "Epoch 495/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7585 - accuracy: 0.6556 - val_loss: 0.7769 - val_accuracy: 0.6516\n",
            "Epoch 496/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7556 - accuracy: 0.6542 - val_loss: 0.7776 - val_accuracy: 0.6581\n",
            "Epoch 497/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7598 - accuracy: 0.6458 - val_loss: 0.7767 - val_accuracy: 0.6516\n",
            "Epoch 498/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7642 - accuracy: 0.6389 - val_loss: 0.7754 - val_accuracy: 0.6452\n",
            "Epoch 499/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7537 - accuracy: 0.6569 - val_loss: 0.7754 - val_accuracy: 0.6452\n",
            "Epoch 500/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7585 - accuracy: 0.6556 - val_loss: 0.7767 - val_accuracy: 0.6516\n",
            "Epoch 501/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7569 - accuracy: 0.6458 - val_loss: 0.7758 - val_accuracy: 0.6516\n",
            "Epoch 502/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7534 - accuracy: 0.6528 - val_loss: 0.7754 - val_accuracy: 0.6516\n",
            "Epoch 503/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7436 - accuracy: 0.6667 - val_loss: 0.7754 - val_accuracy: 0.6581\n",
            "Epoch 504/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7536 - accuracy: 0.6417 - val_loss: 0.7756 - val_accuracy: 0.6581\n",
            "Epoch 505/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7525 - accuracy: 0.6319 - val_loss: 0.7759 - val_accuracy: 0.6516\n",
            "Epoch 506/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7462 - accuracy: 0.6611 - val_loss: 0.7747 - val_accuracy: 0.6581\n",
            "Epoch 507/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7613 - accuracy: 0.6514 - val_loss: 0.7728 - val_accuracy: 0.6516\n",
            "Epoch 508/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7597 - accuracy: 0.6667 - val_loss: 0.7714 - val_accuracy: 0.6581\n",
            "Epoch 509/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7501 - accuracy: 0.6389 - val_loss: 0.7720 - val_accuracy: 0.6581\n",
            "Epoch 510/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7514 - accuracy: 0.6583 - val_loss: 0.7723 - val_accuracy: 0.6516\n",
            "Epoch 511/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7536 - accuracy: 0.6431 - val_loss: 0.7719 - val_accuracy: 0.6516\n",
            "Epoch 512/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7501 - accuracy: 0.6556 - val_loss: 0.7716 - val_accuracy: 0.6645\n",
            "Epoch 513/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7577 - accuracy: 0.6431 - val_loss: 0.7719 - val_accuracy: 0.6516\n",
            "Epoch 514/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7555 - accuracy: 0.6458 - val_loss: 0.7709 - val_accuracy: 0.6645\n",
            "Epoch 515/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7583 - accuracy: 0.6194 - val_loss: 0.7708 - val_accuracy: 0.6645\n",
            "Epoch 516/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7540 - accuracy: 0.6319 - val_loss: 0.7711 - val_accuracy: 0.6516\n",
            "Epoch 517/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7591 - accuracy: 0.6486 - val_loss: 0.7721 - val_accuracy: 0.6645\n",
            "Epoch 518/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7549 - accuracy: 0.6528 - val_loss: 0.7712 - val_accuracy: 0.6645\n",
            "Epoch 519/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7618 - accuracy: 0.6417 - val_loss: 0.7699 - val_accuracy: 0.6581\n",
            "Epoch 520/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7540 - accuracy: 0.6486 - val_loss: 0.7700 - val_accuracy: 0.6516\n",
            "Epoch 521/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7438 - accuracy: 0.6653 - val_loss: 0.7702 - val_accuracy: 0.6516\n",
            "Epoch 522/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7416 - accuracy: 0.6639 - val_loss: 0.7696 - val_accuracy: 0.6452\n",
            "Epoch 523/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7382 - accuracy: 0.6625 - val_loss: 0.7690 - val_accuracy: 0.6581\n",
            "Epoch 524/1000\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.7566 - accuracy: 0.6375 - val_loss: 0.7685 - val_accuracy: 0.6581\n",
            "Epoch 525/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7565 - accuracy: 0.6431 - val_loss: 0.7695 - val_accuracy: 0.6581\n",
            "Epoch 526/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7478 - accuracy: 0.6556 - val_loss: 0.7681 - val_accuracy: 0.6516\n",
            "Epoch 527/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7536 - accuracy: 0.6472 - val_loss: 0.7686 - val_accuracy: 0.6452\n",
            "Epoch 528/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7584 - accuracy: 0.6556 - val_loss: 0.7682 - val_accuracy: 0.6581\n",
            "Epoch 529/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7551 - accuracy: 0.6611 - val_loss: 0.7676 - val_accuracy: 0.6581\n",
            "Epoch 530/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7441 - accuracy: 0.6542 - val_loss: 0.7677 - val_accuracy: 0.6581\n",
            "Epoch 531/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7587 - accuracy: 0.6583 - val_loss: 0.7676 - val_accuracy: 0.6645\n",
            "Epoch 532/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7498 - accuracy: 0.6417 - val_loss: 0.7679 - val_accuracy: 0.6645\n",
            "Epoch 533/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7620 - accuracy: 0.6278 - val_loss: 0.7663 - val_accuracy: 0.6710\n",
            "Epoch 534/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7537 - accuracy: 0.6486 - val_loss: 0.7671 - val_accuracy: 0.6581\n",
            "Epoch 535/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7494 - accuracy: 0.6500 - val_loss: 0.7655 - val_accuracy: 0.6645\n",
            "Epoch 536/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7454 - accuracy: 0.6333 - val_loss: 0.7652 - val_accuracy: 0.6645\n",
            "Epoch 537/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7409 - accuracy: 0.6431 - val_loss: 0.7653 - val_accuracy: 0.6710\n",
            "Epoch 538/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7555 - accuracy: 0.6431 - val_loss: 0.7656 - val_accuracy: 0.6645\n",
            "Epoch 539/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7411 - accuracy: 0.6736 - val_loss: 0.7657 - val_accuracy: 0.6645\n",
            "Epoch 540/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7437 - accuracy: 0.6625 - val_loss: 0.7650 - val_accuracy: 0.6581\n",
            "Epoch 541/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7293 - accuracy: 0.6611 - val_loss: 0.7641 - val_accuracy: 0.6710\n",
            "Epoch 542/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7394 - accuracy: 0.6653 - val_loss: 0.7644 - val_accuracy: 0.6645\n",
            "Epoch 543/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7561 - accuracy: 0.6417 - val_loss: 0.7635 - val_accuracy: 0.6645\n",
            "Epoch 544/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7431 - accuracy: 0.6736 - val_loss: 0.7630 - val_accuracy: 0.6645\n",
            "Epoch 545/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7473 - accuracy: 0.6500 - val_loss: 0.7642 - val_accuracy: 0.6645\n",
            "Epoch 546/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7474 - accuracy: 0.6569 - val_loss: 0.7645 - val_accuracy: 0.6581\n",
            "Epoch 547/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7492 - accuracy: 0.6625 - val_loss: 0.7630 - val_accuracy: 0.6710\n",
            "Epoch 548/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7546 - accuracy: 0.6639 - val_loss: 0.7634 - val_accuracy: 0.6710\n",
            "Epoch 549/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7373 - accuracy: 0.6458 - val_loss: 0.7629 - val_accuracy: 0.6710\n",
            "Epoch 550/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7438 - accuracy: 0.6583 - val_loss: 0.7627 - val_accuracy: 0.6710\n",
            "Epoch 551/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7334 - accuracy: 0.6611 - val_loss: 0.7645 - val_accuracy: 0.6645\n",
            "Epoch 552/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7447 - accuracy: 0.6389 - val_loss: 0.7626 - val_accuracy: 0.6710\n",
            "Epoch 553/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7396 - accuracy: 0.6375 - val_loss: 0.7630 - val_accuracy: 0.6516\n",
            "Epoch 554/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7542 - accuracy: 0.6444 - val_loss: 0.7634 - val_accuracy: 0.6581\n",
            "Epoch 555/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7370 - accuracy: 0.6486 - val_loss: 0.7624 - val_accuracy: 0.6581\n",
            "Epoch 556/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7424 - accuracy: 0.6569 - val_loss: 0.7613 - val_accuracy: 0.6710\n",
            "Epoch 557/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7542 - accuracy: 0.6514 - val_loss: 0.7604 - val_accuracy: 0.6710\n",
            "Epoch 558/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7460 - accuracy: 0.6639 - val_loss: 0.7608 - val_accuracy: 0.6710\n",
            "Epoch 559/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7393 - accuracy: 0.6611 - val_loss: 0.7600 - val_accuracy: 0.6710\n",
            "Epoch 560/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7521 - accuracy: 0.6528 - val_loss: 0.7606 - val_accuracy: 0.6710\n",
            "Epoch 561/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7372 - accuracy: 0.6681 - val_loss: 0.7604 - val_accuracy: 0.6710\n",
            "Epoch 562/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7420 - accuracy: 0.6472 - val_loss: 0.7605 - val_accuracy: 0.6645\n",
            "Epoch 563/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7429 - accuracy: 0.6514 - val_loss: 0.7617 - val_accuracy: 0.6710\n",
            "Epoch 564/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7322 - accuracy: 0.6569 - val_loss: 0.7619 - val_accuracy: 0.6710\n",
            "Epoch 565/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7429 - accuracy: 0.6514 - val_loss: 0.7617 - val_accuracy: 0.6710\n",
            "Epoch 566/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7483 - accuracy: 0.6472 - val_loss: 0.7611 - val_accuracy: 0.6710\n",
            "Epoch 567/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7373 - accuracy: 0.6597 - val_loss: 0.7603 - val_accuracy: 0.6581\n",
            "Epoch 568/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7361 - accuracy: 0.6597 - val_loss: 0.7601 - val_accuracy: 0.6645\n",
            "Epoch 569/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7446 - accuracy: 0.6458 - val_loss: 0.7595 - val_accuracy: 0.6581\n",
            "Epoch 570/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7403 - accuracy: 0.6667 - val_loss: 0.7593 - val_accuracy: 0.6710\n",
            "Epoch 571/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7546 - accuracy: 0.6444 - val_loss: 0.7595 - val_accuracy: 0.6581\n",
            "Epoch 572/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7371 - accuracy: 0.6556 - val_loss: 0.7599 - val_accuracy: 0.6645\n",
            "Epoch 573/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7362 - accuracy: 0.6583 - val_loss: 0.7594 - val_accuracy: 0.6645\n",
            "Epoch 574/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7406 - accuracy: 0.6528 - val_loss: 0.7598 - val_accuracy: 0.6710\n",
            "Epoch 575/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7389 - accuracy: 0.6514 - val_loss: 0.7600 - val_accuracy: 0.6710\n",
            "Epoch 576/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7395 - accuracy: 0.6681 - val_loss: 0.7589 - val_accuracy: 0.6645\n",
            "Epoch 577/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7523 - accuracy: 0.6389 - val_loss: 0.7586 - val_accuracy: 0.6645\n",
            "Epoch 578/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7433 - accuracy: 0.6500 - val_loss: 0.7580 - val_accuracy: 0.6645\n",
            "Epoch 579/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7367 - accuracy: 0.6653 - val_loss: 0.7578 - val_accuracy: 0.6645\n",
            "Epoch 580/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7379 - accuracy: 0.6764 - val_loss: 0.7580 - val_accuracy: 0.6645\n",
            "Epoch 581/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7336 - accuracy: 0.6569 - val_loss: 0.7569 - val_accuracy: 0.6645\n",
            "Epoch 582/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7368 - accuracy: 0.6542 - val_loss: 0.7554 - val_accuracy: 0.6839\n",
            "Epoch 583/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7351 - accuracy: 0.6556 - val_loss: 0.7556 - val_accuracy: 0.6839\n",
            "Epoch 584/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7355 - accuracy: 0.6639 - val_loss: 0.7567 - val_accuracy: 0.6710\n",
            "Epoch 585/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7350 - accuracy: 0.6750 - val_loss: 0.7572 - val_accuracy: 0.6774\n",
            "Epoch 586/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7341 - accuracy: 0.6792 - val_loss: 0.7565 - val_accuracy: 0.6710\n",
            "Epoch 587/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7251 - accuracy: 0.6542 - val_loss: 0.7556 - val_accuracy: 0.6710\n",
            "Epoch 588/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7419 - accuracy: 0.6417 - val_loss: 0.7558 - val_accuracy: 0.6710\n",
            "Epoch 589/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7350 - accuracy: 0.6556 - val_loss: 0.7568 - val_accuracy: 0.6774\n",
            "Epoch 590/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7451 - accuracy: 0.6417 - val_loss: 0.7558 - val_accuracy: 0.6710\n",
            "Epoch 591/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7412 - accuracy: 0.6472 - val_loss: 0.7570 - val_accuracy: 0.6710\n",
            "Epoch 592/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7259 - accuracy: 0.6750 - val_loss: 0.7562 - val_accuracy: 0.6710\n",
            "Epoch 593/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7335 - accuracy: 0.6500 - val_loss: 0.7552 - val_accuracy: 0.6645\n",
            "Epoch 594/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7343 - accuracy: 0.6625 - val_loss: 0.7537 - val_accuracy: 0.6839\n",
            "Epoch 595/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7307 - accuracy: 0.6597 - val_loss: 0.7535 - val_accuracy: 0.6774\n",
            "Epoch 596/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7380 - accuracy: 0.6694 - val_loss: 0.7540 - val_accuracy: 0.6774\n",
            "Epoch 597/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7391 - accuracy: 0.6597 - val_loss: 0.7545 - val_accuracy: 0.6710\n",
            "Epoch 598/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7424 - accuracy: 0.6417 - val_loss: 0.7535 - val_accuracy: 0.6774\n",
            "Epoch 599/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7219 - accuracy: 0.6750 - val_loss: 0.7544 - val_accuracy: 0.6645\n",
            "Epoch 600/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7504 - accuracy: 0.6597 - val_loss: 0.7542 - val_accuracy: 0.6645\n",
            "Epoch 601/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7347 - accuracy: 0.6583 - val_loss: 0.7541 - val_accuracy: 0.6645\n",
            "Epoch 602/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7367 - accuracy: 0.6583 - val_loss: 0.7529 - val_accuracy: 0.6774\n",
            "Epoch 603/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7410 - accuracy: 0.6556 - val_loss: 0.7528 - val_accuracy: 0.6710\n",
            "Epoch 604/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7362 - accuracy: 0.6556 - val_loss: 0.7529 - val_accuracy: 0.6710\n",
            "Epoch 605/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7288 - accuracy: 0.6694 - val_loss: 0.7530 - val_accuracy: 0.6710\n",
            "Epoch 606/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7344 - accuracy: 0.6736 - val_loss: 0.7532 - val_accuracy: 0.6710\n",
            "Epoch 607/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7332 - accuracy: 0.6611 - val_loss: 0.7524 - val_accuracy: 0.6710\n",
            "Epoch 608/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7322 - accuracy: 0.6694 - val_loss: 0.7517 - val_accuracy: 0.6839\n",
            "Epoch 609/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7462 - accuracy: 0.6597 - val_loss: 0.7531 - val_accuracy: 0.6774\n",
            "Epoch 610/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7267 - accuracy: 0.6597 - val_loss: 0.7518 - val_accuracy: 0.6774\n",
            "Epoch 611/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7444 - accuracy: 0.6569 - val_loss: 0.7508 - val_accuracy: 0.6903\n",
            "Epoch 612/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7378 - accuracy: 0.6694 - val_loss: 0.7508 - val_accuracy: 0.6839\n",
            "Epoch 613/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7447 - accuracy: 0.6625 - val_loss: 0.7512 - val_accuracy: 0.6774\n",
            "Epoch 614/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7378 - accuracy: 0.6736 - val_loss: 0.7508 - val_accuracy: 0.6774\n",
            "Epoch 615/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7345 - accuracy: 0.6458 - val_loss: 0.7504 - val_accuracy: 0.6839\n",
            "Epoch 616/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7321 - accuracy: 0.6722 - val_loss: 0.7505 - val_accuracy: 0.6774\n",
            "Epoch 617/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7322 - accuracy: 0.6500 - val_loss: 0.7497 - val_accuracy: 0.6839\n",
            "Epoch 618/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7397 - accuracy: 0.6500 - val_loss: 0.7484 - val_accuracy: 0.6839\n",
            "Epoch 619/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7275 - accuracy: 0.6694 - val_loss: 0.7492 - val_accuracy: 0.6903\n",
            "Epoch 620/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7424 - accuracy: 0.6694 - val_loss: 0.7489 - val_accuracy: 0.6968\n",
            "Epoch 621/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7323 - accuracy: 0.6514 - val_loss: 0.7495 - val_accuracy: 0.6839\n",
            "Epoch 622/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7213 - accuracy: 0.6653 - val_loss: 0.7501 - val_accuracy: 0.6774\n",
            "Epoch 623/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7286 - accuracy: 0.6667 - val_loss: 0.7493 - val_accuracy: 0.6903\n",
            "Epoch 624/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7335 - accuracy: 0.6528 - val_loss: 0.7487 - val_accuracy: 0.6903\n",
            "Epoch 625/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7347 - accuracy: 0.6625 - val_loss: 0.7483 - val_accuracy: 0.6903\n",
            "Epoch 626/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7386 - accuracy: 0.6569 - val_loss: 0.7476 - val_accuracy: 0.6903\n",
            "Epoch 627/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7368 - accuracy: 0.6597 - val_loss: 0.7481 - val_accuracy: 0.6968\n",
            "Epoch 628/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7365 - accuracy: 0.6708 - val_loss: 0.7483 - val_accuracy: 0.6839\n",
            "Epoch 629/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7289 - accuracy: 0.6681 - val_loss: 0.7483 - val_accuracy: 0.6839\n",
            "Epoch 630/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7187 - accuracy: 0.6792 - val_loss: 0.7476 - val_accuracy: 0.6839\n",
            "Epoch 631/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7312 - accuracy: 0.6667 - val_loss: 0.7474 - val_accuracy: 0.6839\n",
            "Epoch 632/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7367 - accuracy: 0.6597 - val_loss: 0.7471 - val_accuracy: 0.6903\n",
            "Epoch 633/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7432 - accuracy: 0.6681 - val_loss: 0.7462 - val_accuracy: 0.7032\n",
            "Epoch 634/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7133 - accuracy: 0.6639 - val_loss: 0.7458 - val_accuracy: 0.6968\n",
            "Epoch 635/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7313 - accuracy: 0.6597 - val_loss: 0.7464 - val_accuracy: 0.6839\n",
            "Epoch 636/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7322 - accuracy: 0.6625 - val_loss: 0.7458 - val_accuracy: 0.6903\n",
            "Epoch 637/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7350 - accuracy: 0.6528 - val_loss: 0.7463 - val_accuracy: 0.6903\n",
            "Epoch 638/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7281 - accuracy: 0.6708 - val_loss: 0.7455 - val_accuracy: 0.6968\n",
            "Epoch 639/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7260 - accuracy: 0.6597 - val_loss: 0.7454 - val_accuracy: 0.6839\n",
            "Epoch 640/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7320 - accuracy: 0.6500 - val_loss: 0.7459 - val_accuracy: 0.6839\n",
            "Epoch 641/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7131 - accuracy: 0.6750 - val_loss: 0.7452 - val_accuracy: 0.6903\n",
            "Epoch 642/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7209 - accuracy: 0.6667 - val_loss: 0.7444 - val_accuracy: 0.6968\n",
            "Epoch 643/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7131 - accuracy: 0.6750 - val_loss: 0.7437 - val_accuracy: 0.7226\n",
            "Epoch 644/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7309 - accuracy: 0.6750 - val_loss: 0.7446 - val_accuracy: 0.6968\n",
            "Epoch 645/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7343 - accuracy: 0.6597 - val_loss: 0.7446 - val_accuracy: 0.6903\n",
            "Epoch 646/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7116 - accuracy: 0.6667 - val_loss: 0.7443 - val_accuracy: 0.6903\n",
            "Epoch 647/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7144 - accuracy: 0.6681 - val_loss: 0.7440 - val_accuracy: 0.6968\n",
            "Epoch 648/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7130 - accuracy: 0.6764 - val_loss: 0.7441 - val_accuracy: 0.6903\n",
            "Epoch 649/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7227 - accuracy: 0.6792 - val_loss: 0.7445 - val_accuracy: 0.6903\n",
            "Epoch 650/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7196 - accuracy: 0.6708 - val_loss: 0.7438 - val_accuracy: 0.6903\n",
            "Epoch 651/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7429 - accuracy: 0.6625 - val_loss: 0.7441 - val_accuracy: 0.6903\n",
            "Epoch 652/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7242 - accuracy: 0.6597 - val_loss: 0.7439 - val_accuracy: 0.6903\n",
            "Epoch 653/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7261 - accuracy: 0.6583 - val_loss: 0.7436 - val_accuracy: 0.6903\n",
            "Epoch 654/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7277 - accuracy: 0.6667 - val_loss: 0.7431 - val_accuracy: 0.6968\n",
            "Epoch 655/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7294 - accuracy: 0.6736 - val_loss: 0.7436 - val_accuracy: 0.6903\n",
            "Epoch 656/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7270 - accuracy: 0.6806 - val_loss: 0.7437 - val_accuracy: 0.6903\n",
            "Epoch 657/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7330 - accuracy: 0.6639 - val_loss: 0.7432 - val_accuracy: 0.6903\n",
            "Epoch 658/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7270 - accuracy: 0.6694 - val_loss: 0.7429 - val_accuracy: 0.6903\n",
            "Epoch 659/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7167 - accuracy: 0.6556 - val_loss: 0.7425 - val_accuracy: 0.6903\n",
            "Epoch 660/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7292 - accuracy: 0.6708 - val_loss: 0.7429 - val_accuracy: 0.6903\n",
            "Epoch 661/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7327 - accuracy: 0.6625 - val_loss: 0.7432 - val_accuracy: 0.6968\n",
            "Epoch 662/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7287 - accuracy: 0.6778 - val_loss: 0.7437 - val_accuracy: 0.6903\n",
            "Epoch 663/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7201 - accuracy: 0.6653 - val_loss: 0.7427 - val_accuracy: 0.6903\n",
            "Epoch 664/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7336 - accuracy: 0.6639 - val_loss: 0.7418 - val_accuracy: 0.6903\n",
            "Epoch 665/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7324 - accuracy: 0.6778 - val_loss: 0.7439 - val_accuracy: 0.6903\n",
            "Epoch 666/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7478 - accuracy: 0.6431 - val_loss: 0.7441 - val_accuracy: 0.6839\n",
            "Epoch 667/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7252 - accuracy: 0.6528 - val_loss: 0.7431 - val_accuracy: 0.6839\n",
            "Epoch 668/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7200 - accuracy: 0.6875 - val_loss: 0.7429 - val_accuracy: 0.6903\n",
            "Epoch 669/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7220 - accuracy: 0.6597 - val_loss: 0.7432 - val_accuracy: 0.6839\n",
            "Epoch 670/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7146 - accuracy: 0.6722 - val_loss: 0.7432 - val_accuracy: 0.6839\n",
            "Epoch 671/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7231 - accuracy: 0.6819 - val_loss: 0.7424 - val_accuracy: 0.6903\n",
            "Epoch 672/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7254 - accuracy: 0.6639 - val_loss: 0.7416 - val_accuracy: 0.6968\n",
            "Epoch 673/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7187 - accuracy: 0.6806 - val_loss: 0.7407 - val_accuracy: 0.7097\n",
            "Epoch 674/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7324 - accuracy: 0.6694 - val_loss: 0.7410 - val_accuracy: 0.6903\n",
            "Epoch 675/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7123 - accuracy: 0.6806 - val_loss: 0.7413 - val_accuracy: 0.6968\n",
            "Epoch 676/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7276 - accuracy: 0.6625 - val_loss: 0.7405 - val_accuracy: 0.6903\n",
            "Epoch 677/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7118 - accuracy: 0.6792 - val_loss: 0.7404 - val_accuracy: 0.6968\n",
            "Epoch 678/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7254 - accuracy: 0.6597 - val_loss: 0.7412 - val_accuracy: 0.6968\n",
            "Epoch 679/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7160 - accuracy: 0.6625 - val_loss: 0.7404 - val_accuracy: 0.6968\n",
            "Epoch 680/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7220 - accuracy: 0.6417 - val_loss: 0.7392 - val_accuracy: 0.7161\n",
            "Epoch 681/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7115 - accuracy: 0.6681 - val_loss: 0.7389 - val_accuracy: 0.7226\n",
            "Epoch 682/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7128 - accuracy: 0.6833 - val_loss: 0.7381 - val_accuracy: 0.7355\n",
            "Epoch 683/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7249 - accuracy: 0.6486 - val_loss: 0.7384 - val_accuracy: 0.7290\n",
            "Epoch 684/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7262 - accuracy: 0.6500 - val_loss: 0.7382 - val_accuracy: 0.7290\n",
            "Epoch 685/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7168 - accuracy: 0.6639 - val_loss: 0.7391 - val_accuracy: 0.7161\n",
            "Epoch 686/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7088 - accuracy: 0.6722 - val_loss: 0.7395 - val_accuracy: 0.6903\n",
            "Epoch 687/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7344 - accuracy: 0.6708 - val_loss: 0.7390 - val_accuracy: 0.7032\n",
            "Epoch 688/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7206 - accuracy: 0.6583 - val_loss: 0.7390 - val_accuracy: 0.6968\n",
            "Epoch 689/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7249 - accuracy: 0.6542 - val_loss: 0.7384 - val_accuracy: 0.7032\n",
            "Epoch 690/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7289 - accuracy: 0.6653 - val_loss: 0.7375 - val_accuracy: 0.7226\n",
            "Epoch 691/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7235 - accuracy: 0.6583 - val_loss: 0.7379 - val_accuracy: 0.7226\n",
            "Epoch 692/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7111 - accuracy: 0.6792 - val_loss: 0.7382 - val_accuracy: 0.7226\n",
            "Epoch 693/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7245 - accuracy: 0.6708 - val_loss: 0.7367 - val_accuracy: 0.7290\n",
            "Epoch 694/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7221 - accuracy: 0.6764 - val_loss: 0.7367 - val_accuracy: 0.7290\n",
            "Epoch 695/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7373 - accuracy: 0.6556 - val_loss: 0.7362 - val_accuracy: 0.7290\n",
            "Epoch 696/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7227 - accuracy: 0.6528 - val_loss: 0.7360 - val_accuracy: 0.7290\n",
            "Epoch 697/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7277 - accuracy: 0.6403 - val_loss: 0.7368 - val_accuracy: 0.7355\n",
            "Epoch 698/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7267 - accuracy: 0.6819 - val_loss: 0.7360 - val_accuracy: 0.7355\n",
            "Epoch 699/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7189 - accuracy: 0.6694 - val_loss: 0.7355 - val_accuracy: 0.7290\n",
            "Epoch 700/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7317 - accuracy: 0.6458 - val_loss: 0.7347 - val_accuracy: 0.7355\n",
            "Epoch 701/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7229 - accuracy: 0.6792 - val_loss: 0.7358 - val_accuracy: 0.7355\n",
            "Epoch 702/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7177 - accuracy: 0.6694 - val_loss: 0.7363 - val_accuracy: 0.7226\n",
            "Epoch 703/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7119 - accuracy: 0.6750 - val_loss: 0.7352 - val_accuracy: 0.7355\n",
            "Epoch 704/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7260 - accuracy: 0.6667 - val_loss: 0.7347 - val_accuracy: 0.7290\n",
            "Epoch 705/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7076 - accuracy: 0.6819 - val_loss: 0.7342 - val_accuracy: 0.7290\n",
            "Epoch 706/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7149 - accuracy: 0.6750 - val_loss: 0.7348 - val_accuracy: 0.7290\n",
            "Epoch 707/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7254 - accuracy: 0.6528 - val_loss: 0.7346 - val_accuracy: 0.7290\n",
            "Epoch 708/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7105 - accuracy: 0.6889 - val_loss: 0.7354 - val_accuracy: 0.7226\n",
            "Epoch 709/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7064 - accuracy: 0.6569 - val_loss: 0.7350 - val_accuracy: 0.7226\n",
            "Epoch 710/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7287 - accuracy: 0.6556 - val_loss: 0.7352 - val_accuracy: 0.7161\n",
            "Epoch 711/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7048 - accuracy: 0.6875 - val_loss: 0.7350 - val_accuracy: 0.7226\n",
            "Epoch 712/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7179 - accuracy: 0.6736 - val_loss: 0.7345 - val_accuracy: 0.7161\n",
            "Epoch 713/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7224 - accuracy: 0.6569 - val_loss: 0.7347 - val_accuracy: 0.7097\n",
            "Epoch 714/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7236 - accuracy: 0.6750 - val_loss: 0.7347 - val_accuracy: 0.7226\n",
            "Epoch 715/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7176 - accuracy: 0.6639 - val_loss: 0.7338 - val_accuracy: 0.7290\n",
            "Epoch 716/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7212 - accuracy: 0.6667 - val_loss: 0.7329 - val_accuracy: 0.7290\n",
            "Epoch 717/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7289 - accuracy: 0.6472 - val_loss: 0.7331 - val_accuracy: 0.7290\n",
            "Epoch 718/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7329 - accuracy: 0.6736 - val_loss: 0.7332 - val_accuracy: 0.7355\n",
            "Epoch 719/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7206 - accuracy: 0.6764 - val_loss: 0.7318 - val_accuracy: 0.7355\n",
            "Epoch 720/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7193 - accuracy: 0.6708 - val_loss: 0.7327 - val_accuracy: 0.7226\n",
            "Epoch 721/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7159 - accuracy: 0.6778 - val_loss: 0.7332 - val_accuracy: 0.7226\n",
            "Epoch 722/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7270 - accuracy: 0.6569 - val_loss: 0.7329 - val_accuracy: 0.7226\n",
            "Epoch 723/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7189 - accuracy: 0.6750 - val_loss: 0.7333 - val_accuracy: 0.7161\n",
            "Epoch 724/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7154 - accuracy: 0.6611 - val_loss: 0.7345 - val_accuracy: 0.7032\n",
            "Epoch 725/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7111 - accuracy: 0.6681 - val_loss: 0.7334 - val_accuracy: 0.7161\n",
            "Epoch 726/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7171 - accuracy: 0.6736 - val_loss: 0.7336 - val_accuracy: 0.7226\n",
            "Epoch 727/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7080 - accuracy: 0.6667 - val_loss: 0.7328 - val_accuracy: 0.7161\n",
            "Epoch 728/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7252 - accuracy: 0.6514 - val_loss: 0.7327 - val_accuracy: 0.7226\n",
            "Epoch 729/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7130 - accuracy: 0.6833 - val_loss: 0.7325 - val_accuracy: 0.7290\n",
            "Epoch 730/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7106 - accuracy: 0.6653 - val_loss: 0.7323 - val_accuracy: 0.7290\n",
            "Epoch 731/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7170 - accuracy: 0.6778 - val_loss: 0.7325 - val_accuracy: 0.7290\n",
            "Epoch 732/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7186 - accuracy: 0.6778 - val_loss: 0.7346 - val_accuracy: 0.7097\n",
            "Epoch 733/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7187 - accuracy: 0.6708 - val_loss: 0.7347 - val_accuracy: 0.7097\n",
            "Epoch 734/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7213 - accuracy: 0.6778 - val_loss: 0.7329 - val_accuracy: 0.7161\n",
            "Epoch 735/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7225 - accuracy: 0.6722 - val_loss: 0.7324 - val_accuracy: 0.7226\n",
            "Epoch 736/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7127 - accuracy: 0.6486 - val_loss: 0.7328 - val_accuracy: 0.7097\n",
            "Epoch 737/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7084 - accuracy: 0.6708 - val_loss: 0.7325 - val_accuracy: 0.7226\n",
            "Epoch 738/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7162 - accuracy: 0.6847 - val_loss: 0.7303 - val_accuracy: 0.7355\n",
            "Epoch 739/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7082 - accuracy: 0.6792 - val_loss: 0.7311 - val_accuracy: 0.7290\n",
            "Epoch 740/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7172 - accuracy: 0.6569 - val_loss: 0.7306 - val_accuracy: 0.7290\n",
            "Epoch 741/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7252 - accuracy: 0.6500 - val_loss: 0.7305 - val_accuracy: 0.7290\n",
            "Epoch 742/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7107 - accuracy: 0.6778 - val_loss: 0.7308 - val_accuracy: 0.7290\n",
            "Epoch 743/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7304 - accuracy: 0.6639 - val_loss: 0.7313 - val_accuracy: 0.7226\n",
            "Epoch 744/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7083 - accuracy: 0.6819 - val_loss: 0.7304 - val_accuracy: 0.7290\n",
            "Epoch 745/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7093 - accuracy: 0.6736 - val_loss: 0.7288 - val_accuracy: 0.7226\n",
            "Epoch 746/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7151 - accuracy: 0.6764 - val_loss: 0.7296 - val_accuracy: 0.7290\n",
            "Epoch 747/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7132 - accuracy: 0.6792 - val_loss: 0.7290 - val_accuracy: 0.7161\n",
            "Epoch 748/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7231 - accuracy: 0.6833 - val_loss: 0.7292 - val_accuracy: 0.7161\n",
            "Epoch 749/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7181 - accuracy: 0.6708 - val_loss: 0.7290 - val_accuracy: 0.7161\n",
            "Epoch 750/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7113 - accuracy: 0.6875 - val_loss: 0.7296 - val_accuracy: 0.7161\n",
            "Epoch 751/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7075 - accuracy: 0.6667 - val_loss: 0.7301 - val_accuracy: 0.7226\n",
            "Epoch 752/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7212 - accuracy: 0.6750 - val_loss: 0.7294 - val_accuracy: 0.7226\n",
            "Epoch 753/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7170 - accuracy: 0.6639 - val_loss: 0.7293 - val_accuracy: 0.7226\n",
            "Epoch 754/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7186 - accuracy: 0.6764 - val_loss: 0.7289 - val_accuracy: 0.7226\n",
            "Epoch 755/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7127 - accuracy: 0.6722 - val_loss: 0.7279 - val_accuracy: 0.7290\n",
            "Epoch 756/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7089 - accuracy: 0.6681 - val_loss: 0.7275 - val_accuracy: 0.7355\n",
            "Epoch 757/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7155 - accuracy: 0.6583 - val_loss: 0.7276 - val_accuracy: 0.7355\n",
            "Epoch 758/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7126 - accuracy: 0.6708 - val_loss: 0.7272 - val_accuracy: 0.7355\n",
            "Epoch 759/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7054 - accuracy: 0.6764 - val_loss: 0.7276 - val_accuracy: 0.7290\n",
            "Epoch 760/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7179 - accuracy: 0.6722 - val_loss: 0.7279 - val_accuracy: 0.7290\n",
            "Epoch 761/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7181 - accuracy: 0.6569 - val_loss: 0.7274 - val_accuracy: 0.7290\n",
            "Epoch 762/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7092 - accuracy: 0.6806 - val_loss: 0.7287 - val_accuracy: 0.7161\n",
            "Epoch 763/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7040 - accuracy: 0.6847 - val_loss: 0.7284 - val_accuracy: 0.7161\n",
            "Epoch 764/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7184 - accuracy: 0.6625 - val_loss: 0.7271 - val_accuracy: 0.7290\n",
            "Epoch 765/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7190 - accuracy: 0.6542 - val_loss: 0.7284 - val_accuracy: 0.7097\n",
            "Epoch 766/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7172 - accuracy: 0.6583 - val_loss: 0.7282 - val_accuracy: 0.7097\n",
            "Epoch 767/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6987 - accuracy: 0.6875 - val_loss: 0.7281 - val_accuracy: 0.7097\n",
            "Epoch 768/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7008 - accuracy: 0.6778 - val_loss: 0.7266 - val_accuracy: 0.7290\n",
            "Epoch 769/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7155 - accuracy: 0.6569 - val_loss: 0.7262 - val_accuracy: 0.7290\n",
            "Epoch 770/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7089 - accuracy: 0.6792 - val_loss: 0.7264 - val_accuracy: 0.7290\n",
            "Epoch 771/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7195 - accuracy: 0.6750 - val_loss: 0.7256 - val_accuracy: 0.7290\n",
            "Epoch 772/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7157 - accuracy: 0.6778 - val_loss: 0.7265 - val_accuracy: 0.7226\n",
            "Epoch 773/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7089 - accuracy: 0.6736 - val_loss: 0.7248 - val_accuracy: 0.7290\n",
            "Epoch 774/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7204 - accuracy: 0.6556 - val_loss: 0.7256 - val_accuracy: 0.7290\n",
            "Epoch 775/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7087 - accuracy: 0.6792 - val_loss: 0.7248 - val_accuracy: 0.7290\n",
            "Epoch 776/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7103 - accuracy: 0.6736 - val_loss: 0.7248 - val_accuracy: 0.7290\n",
            "Epoch 777/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.6889 - val_loss: 0.7247 - val_accuracy: 0.7290\n",
            "Epoch 778/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7142 - accuracy: 0.6639 - val_loss: 0.7255 - val_accuracy: 0.7290\n",
            "Epoch 779/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7046 - accuracy: 0.6764 - val_loss: 0.7244 - val_accuracy: 0.7290\n",
            "Epoch 780/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7073 - accuracy: 0.6806 - val_loss: 0.7243 - val_accuracy: 0.7290\n",
            "Epoch 781/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7065 - accuracy: 0.6875 - val_loss: 0.7245 - val_accuracy: 0.7290\n",
            "Epoch 782/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7077 - accuracy: 0.6917 - val_loss: 0.7240 - val_accuracy: 0.7290\n",
            "Epoch 783/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7073 - accuracy: 0.6833 - val_loss: 0.7241 - val_accuracy: 0.7355\n",
            "Epoch 784/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7025 - accuracy: 0.6778 - val_loss: 0.7246 - val_accuracy: 0.7290\n",
            "Epoch 785/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7103 - accuracy: 0.6611 - val_loss: 0.7250 - val_accuracy: 0.7226\n",
            "Epoch 786/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7093 - accuracy: 0.6736 - val_loss: 0.7242 - val_accuracy: 0.7290\n",
            "Epoch 787/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7114 - accuracy: 0.6806 - val_loss: 0.7241 - val_accuracy: 0.7290\n",
            "Epoch 788/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7155 - accuracy: 0.6708 - val_loss: 0.7255 - val_accuracy: 0.7226\n",
            "Epoch 789/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7134 - accuracy: 0.6833 - val_loss: 0.7254 - val_accuracy: 0.7226\n",
            "Epoch 790/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7056 - accuracy: 0.6792 - val_loss: 0.7242 - val_accuracy: 0.7290\n",
            "Epoch 791/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7107 - accuracy: 0.6722 - val_loss: 0.7231 - val_accuracy: 0.7290\n",
            "Epoch 792/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7048 - accuracy: 0.6736 - val_loss: 0.7238 - val_accuracy: 0.7290\n",
            "Epoch 793/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7208 - accuracy: 0.6625 - val_loss: 0.7230 - val_accuracy: 0.7290\n",
            "Epoch 794/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7116 - accuracy: 0.6736 - val_loss: 0.7237 - val_accuracy: 0.7290\n",
            "Epoch 795/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7021 - accuracy: 0.6792 - val_loss: 0.7235 - val_accuracy: 0.7290\n",
            "Epoch 796/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7113 - accuracy: 0.6722 - val_loss: 0.7221 - val_accuracy: 0.7355\n",
            "Epoch 797/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7116 - accuracy: 0.6681 - val_loss: 0.7222 - val_accuracy: 0.7355\n",
            "Epoch 798/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7125 - accuracy: 0.6556 - val_loss: 0.7221 - val_accuracy: 0.7290\n",
            "Epoch 799/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7134 - accuracy: 0.6583 - val_loss: 0.7223 - val_accuracy: 0.7290\n",
            "Epoch 800/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7107 - accuracy: 0.6764 - val_loss: 0.7229 - val_accuracy: 0.7290\n",
            "Epoch 801/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6952 - accuracy: 0.6792 - val_loss: 0.7231 - val_accuracy: 0.7290\n",
            "Epoch 802/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7056 - accuracy: 0.6792 - val_loss: 0.7242 - val_accuracy: 0.7226\n",
            "Epoch 803/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7193 - accuracy: 0.6556 - val_loss: 0.7227 - val_accuracy: 0.7290\n",
            "Epoch 804/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7083 - accuracy: 0.6722 - val_loss: 0.7234 - val_accuracy: 0.7226\n",
            "Epoch 805/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7130 - accuracy: 0.6639 - val_loss: 0.7231 - val_accuracy: 0.7226\n",
            "Epoch 806/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7050 - accuracy: 0.6694 - val_loss: 0.7219 - val_accuracy: 0.7290\n",
            "Epoch 807/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7095 - accuracy: 0.6722 - val_loss: 0.7226 - val_accuracy: 0.7226\n",
            "Epoch 808/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7127 - accuracy: 0.6806 - val_loss: 0.7226 - val_accuracy: 0.7226\n",
            "Epoch 809/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7143 - accuracy: 0.6736 - val_loss: 0.7228 - val_accuracy: 0.7226\n",
            "Epoch 810/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7124 - accuracy: 0.6736 - val_loss: 0.7213 - val_accuracy: 0.7355\n",
            "Epoch 811/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7063 - accuracy: 0.6847 - val_loss: 0.7210 - val_accuracy: 0.7355\n",
            "Epoch 812/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7046 - accuracy: 0.6778 - val_loss: 0.7211 - val_accuracy: 0.7290\n",
            "Epoch 813/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6980 - accuracy: 0.6722 - val_loss: 0.7213 - val_accuracy: 0.7290\n",
            "Epoch 814/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7077 - accuracy: 0.6722 - val_loss: 0.7215 - val_accuracy: 0.7226\n",
            "Epoch 815/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7008 - accuracy: 0.6792 - val_loss: 0.7200 - val_accuracy: 0.7355\n",
            "Epoch 816/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7038 - accuracy: 0.6778 - val_loss: 0.7202 - val_accuracy: 0.7290\n",
            "Epoch 817/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7072 - accuracy: 0.6694 - val_loss: 0.7208 - val_accuracy: 0.7226\n",
            "Epoch 818/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7055 - accuracy: 0.6708 - val_loss: 0.7209 - val_accuracy: 0.7226\n",
            "Epoch 819/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6971 - accuracy: 0.6833 - val_loss: 0.7195 - val_accuracy: 0.7355\n",
            "Epoch 820/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7116 - accuracy: 0.6569 - val_loss: 0.7190 - val_accuracy: 0.7355\n",
            "Epoch 821/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6954 - accuracy: 0.6819 - val_loss: 0.7203 - val_accuracy: 0.7226\n",
            "Epoch 822/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7152 - accuracy: 0.6792 - val_loss: 0.7212 - val_accuracy: 0.7226\n",
            "Epoch 823/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7089 - accuracy: 0.6708 - val_loss: 0.7200 - val_accuracy: 0.7290\n",
            "Epoch 824/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.6736 - val_loss: 0.7197 - val_accuracy: 0.7290\n",
            "Epoch 825/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6907 - accuracy: 0.6722 - val_loss: 0.7191 - val_accuracy: 0.7290\n",
            "Epoch 826/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7081 - accuracy: 0.6917 - val_loss: 0.7195 - val_accuracy: 0.7290\n",
            "Epoch 827/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6879 - accuracy: 0.6764 - val_loss: 0.7195 - val_accuracy: 0.7226\n",
            "Epoch 828/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7057 - accuracy: 0.6847 - val_loss: 0.7183 - val_accuracy: 0.7290\n",
            "Epoch 829/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7105 - accuracy: 0.6569 - val_loss: 0.7181 - val_accuracy: 0.7290\n",
            "Epoch 830/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7131 - accuracy: 0.6708 - val_loss: 0.7203 - val_accuracy: 0.7226\n",
            "Epoch 831/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7035 - accuracy: 0.6875 - val_loss: 0.7199 - val_accuracy: 0.7226\n",
            "Epoch 832/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6985 - accuracy: 0.6806 - val_loss: 0.7199 - val_accuracy: 0.7161\n",
            "Epoch 833/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7025 - accuracy: 0.6722 - val_loss: 0.7191 - val_accuracy: 0.7161\n",
            "Epoch 834/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7010 - accuracy: 0.6750 - val_loss: 0.7177 - val_accuracy: 0.7290\n",
            "Epoch 835/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7019 - accuracy: 0.6764 - val_loss: 0.7179 - val_accuracy: 0.7290\n",
            "Epoch 836/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6944 - accuracy: 0.6750 - val_loss: 0.7171 - val_accuracy: 0.7290\n",
            "Epoch 837/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7143 - accuracy: 0.6681 - val_loss: 0.7172 - val_accuracy: 0.7290\n",
            "Epoch 838/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6971 - accuracy: 0.6931 - val_loss: 0.7176 - val_accuracy: 0.7226\n",
            "Epoch 839/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7084 - accuracy: 0.6528 - val_loss: 0.7163 - val_accuracy: 0.7290\n",
            "Epoch 840/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6884 - accuracy: 0.6764 - val_loss: 0.7159 - val_accuracy: 0.7290\n",
            "Epoch 841/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7043 - accuracy: 0.6708 - val_loss: 0.7152 - val_accuracy: 0.7290\n",
            "Epoch 842/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7035 - accuracy: 0.6778 - val_loss: 0.7158 - val_accuracy: 0.7290\n",
            "Epoch 843/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7050 - accuracy: 0.6722 - val_loss: 0.7163 - val_accuracy: 0.7290\n",
            "Epoch 844/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7034 - accuracy: 0.6639 - val_loss: 0.7167 - val_accuracy: 0.7290\n",
            "Epoch 845/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7065 - accuracy: 0.6917 - val_loss: 0.7161 - val_accuracy: 0.7290\n",
            "Epoch 846/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6927 - accuracy: 0.6694 - val_loss: 0.7155 - val_accuracy: 0.7290\n",
            "Epoch 847/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7085 - accuracy: 0.6917 - val_loss: 0.7152 - val_accuracy: 0.7290\n",
            "Epoch 848/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7111 - accuracy: 0.6597 - val_loss: 0.7161 - val_accuracy: 0.7290\n",
            "Epoch 849/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7089 - accuracy: 0.6667 - val_loss: 0.7152 - val_accuracy: 0.7355\n",
            "Epoch 850/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7062 - accuracy: 0.6750 - val_loss: 0.7160 - val_accuracy: 0.7290\n",
            "Epoch 851/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7181 - accuracy: 0.6694 - val_loss: 0.7159 - val_accuracy: 0.7355\n",
            "Epoch 852/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6978 - accuracy: 0.6931 - val_loss: 0.7154 - val_accuracy: 0.7355\n",
            "Epoch 853/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6989 - accuracy: 0.7000 - val_loss: 0.7155 - val_accuracy: 0.7355\n",
            "Epoch 854/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7128 - accuracy: 0.6972 - val_loss: 0.7159 - val_accuracy: 0.7290\n",
            "Epoch 855/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7073 - accuracy: 0.6736 - val_loss: 0.7156 - val_accuracy: 0.7355\n",
            "Epoch 856/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6960 - accuracy: 0.6667 - val_loss: 0.7152 - val_accuracy: 0.7355\n",
            "Epoch 857/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6978 - accuracy: 0.6778 - val_loss: 0.7160 - val_accuracy: 0.7290\n",
            "Epoch 858/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7032 - accuracy: 0.6889 - val_loss: 0.7155 - val_accuracy: 0.7355\n",
            "Epoch 859/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7003 - accuracy: 0.6903 - val_loss: 0.7147 - val_accuracy: 0.7355\n",
            "Epoch 860/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7032 - accuracy: 0.6806 - val_loss: 0.7140 - val_accuracy: 0.7355\n",
            "Epoch 861/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7013 - accuracy: 0.6819 - val_loss: 0.7141 - val_accuracy: 0.7355\n",
            "Epoch 862/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7039 - accuracy: 0.6986 - val_loss: 0.7142 - val_accuracy: 0.7290\n",
            "Epoch 863/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.6986 - val_loss: 0.7135 - val_accuracy: 0.7290\n",
            "Epoch 864/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.6972 - val_loss: 0.7142 - val_accuracy: 0.7290\n",
            "Epoch 865/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6963 - accuracy: 0.6833 - val_loss: 0.7135 - val_accuracy: 0.7290\n",
            "Epoch 866/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6996 - accuracy: 0.6944 - val_loss: 0.7137 - val_accuracy: 0.7290\n",
            "Epoch 867/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7131 - accuracy: 0.6653 - val_loss: 0.7150 - val_accuracy: 0.7161\n",
            "Epoch 868/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6909 - accuracy: 0.6764 - val_loss: 0.7147 - val_accuracy: 0.7161\n",
            "Epoch 869/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6919 - accuracy: 0.6792 - val_loss: 0.7135 - val_accuracy: 0.7290\n",
            "Epoch 870/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7040 - accuracy: 0.6847 - val_loss: 0.7126 - val_accuracy: 0.7290\n",
            "Epoch 871/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6896 - accuracy: 0.6903 - val_loss: 0.7125 - val_accuracy: 0.7290\n",
            "Epoch 872/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6832 - accuracy: 0.6847 - val_loss: 0.7126 - val_accuracy: 0.7290\n",
            "Epoch 873/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6996 - accuracy: 0.6819 - val_loss: 0.7124 - val_accuracy: 0.7290\n",
            "Epoch 874/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6947 - accuracy: 0.6847 - val_loss: 0.7124 - val_accuracy: 0.7290\n",
            "Epoch 875/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7018 - accuracy: 0.6792 - val_loss: 0.7132 - val_accuracy: 0.7290\n",
            "Epoch 876/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6964 - accuracy: 0.6722 - val_loss: 0.7124 - val_accuracy: 0.7290\n",
            "Epoch 877/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6952 - accuracy: 0.6722 - val_loss: 0.7127 - val_accuracy: 0.7290\n",
            "Epoch 878/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.6944 - val_loss: 0.7120 - val_accuracy: 0.7355\n",
            "Epoch 879/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.6861 - val_loss: 0.7118 - val_accuracy: 0.7290\n",
            "Epoch 880/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6890 - accuracy: 0.7014 - val_loss: 0.7117 - val_accuracy: 0.7290\n",
            "Epoch 881/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6991 - accuracy: 0.7000 - val_loss: 0.7109 - val_accuracy: 0.7419\n",
            "Epoch 882/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7040 - accuracy: 0.6903 - val_loss: 0.7126 - val_accuracy: 0.7290\n",
            "Epoch 883/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6876 - accuracy: 0.6889 - val_loss: 0.7112 - val_accuracy: 0.7355\n",
            "Epoch 884/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6968 - accuracy: 0.6819 - val_loss: 0.7108 - val_accuracy: 0.7355\n",
            "Epoch 885/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7084 - accuracy: 0.6764 - val_loss: 0.7118 - val_accuracy: 0.7290\n",
            "Epoch 886/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7068 - accuracy: 0.6792 - val_loss: 0.7124 - val_accuracy: 0.7226\n",
            "Epoch 887/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7031 - accuracy: 0.6722 - val_loss: 0.7126 - val_accuracy: 0.7226\n",
            "Epoch 888/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7077 - accuracy: 0.6722 - val_loss: 0.7110 - val_accuracy: 0.7355\n",
            "Epoch 889/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7075 - accuracy: 0.6625 - val_loss: 0.7110 - val_accuracy: 0.7355\n",
            "Epoch 890/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7025 - accuracy: 0.6875 - val_loss: 0.7099 - val_accuracy: 0.7355\n",
            "Epoch 891/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6950 - accuracy: 0.6875 - val_loss: 0.7099 - val_accuracy: 0.7355\n",
            "Epoch 892/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6891 - accuracy: 0.6806 - val_loss: 0.7101 - val_accuracy: 0.7355\n",
            "Epoch 893/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7026 - accuracy: 0.6778 - val_loss: 0.7099 - val_accuracy: 0.7355\n",
            "Epoch 894/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6886 - accuracy: 0.7056 - val_loss: 0.7096 - val_accuracy: 0.7355\n",
            "Epoch 895/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6973 - accuracy: 0.6653 - val_loss: 0.7098 - val_accuracy: 0.7355\n",
            "Epoch 896/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6950 - accuracy: 0.6861 - val_loss: 0.7106 - val_accuracy: 0.7226\n",
            "Epoch 897/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7048 - accuracy: 0.6764 - val_loss: 0.7101 - val_accuracy: 0.7355\n",
            "Epoch 898/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6978 - accuracy: 0.6847 - val_loss: 0.7099 - val_accuracy: 0.7290\n",
            "Epoch 899/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6894 - accuracy: 0.6708 - val_loss: 0.7087 - val_accuracy: 0.7355\n",
            "Epoch 900/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6892 - accuracy: 0.6875 - val_loss: 0.7091 - val_accuracy: 0.7290\n",
            "Epoch 901/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.6931 - val_loss: 0.7093 - val_accuracy: 0.7290\n",
            "Epoch 902/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7001 - accuracy: 0.6653 - val_loss: 0.7096 - val_accuracy: 0.7290\n",
            "Epoch 903/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6992 - accuracy: 0.6778 - val_loss: 0.7096 - val_accuracy: 0.7290\n",
            "Epoch 904/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6797 - accuracy: 0.6917 - val_loss: 0.7097 - val_accuracy: 0.7290\n",
            "Epoch 905/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6891 - accuracy: 0.6917 - val_loss: 0.7093 - val_accuracy: 0.7290\n",
            "Epoch 906/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6913 - accuracy: 0.6917 - val_loss: 0.7099 - val_accuracy: 0.7290\n",
            "Epoch 907/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7064 - accuracy: 0.6556 - val_loss: 0.7115 - val_accuracy: 0.7290\n",
            "Epoch 908/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6817 - accuracy: 0.7056 - val_loss: 0.7108 - val_accuracy: 0.7226\n",
            "Epoch 909/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6914 - accuracy: 0.7000 - val_loss: 0.7086 - val_accuracy: 0.7290\n",
            "Epoch 910/1000\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7003 - accuracy: 0.6917 - val_loss: 0.7086 - val_accuracy: 0.7355\n",
            "Epoch 911/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6983 - accuracy: 0.6708 - val_loss: 0.7085 - val_accuracy: 0.7355\n",
            "Epoch 912/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6948 - accuracy: 0.6736 - val_loss: 0.7093 - val_accuracy: 0.7290\n",
            "Epoch 913/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.6958 - val_loss: 0.7093 - val_accuracy: 0.7290\n",
            "Epoch 914/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6976 - accuracy: 0.6750 - val_loss: 0.7086 - val_accuracy: 0.7290\n",
            "Epoch 915/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6872 - accuracy: 0.7111 - val_loss: 0.7075 - val_accuracy: 0.7355\n",
            "Epoch 916/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7008 - accuracy: 0.6875 - val_loss: 0.7087 - val_accuracy: 0.7290\n",
            "Epoch 917/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7028 - accuracy: 0.6542 - val_loss: 0.7081 - val_accuracy: 0.7290\n",
            "Epoch 918/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6877 - accuracy: 0.6917 - val_loss: 0.7080 - val_accuracy: 0.7290\n",
            "Epoch 919/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7039 - accuracy: 0.6861 - val_loss: 0.7088 - val_accuracy: 0.7290\n",
            "Epoch 920/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6908 - accuracy: 0.6903 - val_loss: 0.7079 - val_accuracy: 0.7290\n",
            "Epoch 921/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6929 - accuracy: 0.6833 - val_loss: 0.7079 - val_accuracy: 0.7290\n",
            "Epoch 922/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6970 - accuracy: 0.6931 - val_loss: 0.7064 - val_accuracy: 0.7355\n",
            "Epoch 923/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6949 - accuracy: 0.6847 - val_loss: 0.7070 - val_accuracy: 0.7290\n",
            "Epoch 924/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.6847 - val_loss: 0.7057 - val_accuracy: 0.7419\n",
            "Epoch 925/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7033 - accuracy: 0.6681 - val_loss: 0.7062 - val_accuracy: 0.7355\n",
            "Epoch 926/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6870 - accuracy: 0.6833 - val_loss: 0.7062 - val_accuracy: 0.7355\n",
            "Epoch 927/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6903 - accuracy: 0.6986 - val_loss: 0.7060 - val_accuracy: 0.7355\n",
            "Epoch 928/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.7014 - val_loss: 0.7053 - val_accuracy: 0.7355\n",
            "Epoch 929/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6917 - accuracy: 0.6847 - val_loss: 0.7062 - val_accuracy: 0.7290\n",
            "Epoch 930/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7048 - accuracy: 0.6833 - val_loss: 0.7056 - val_accuracy: 0.7355\n",
            "Epoch 931/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.6694 - val_loss: 0.7053 - val_accuracy: 0.7355\n",
            "Epoch 932/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6945 - accuracy: 0.6694 - val_loss: 0.7066 - val_accuracy: 0.7290\n",
            "Epoch 933/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6943 - accuracy: 0.6778 - val_loss: 0.7059 - val_accuracy: 0.7290\n",
            "Epoch 934/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6973 - accuracy: 0.6806 - val_loss: 0.7056 - val_accuracy: 0.7290\n",
            "Epoch 935/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6975 - accuracy: 0.6694 - val_loss: 0.7066 - val_accuracy: 0.7226\n",
            "Epoch 936/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7011 - accuracy: 0.6708 - val_loss: 0.7072 - val_accuracy: 0.7226\n",
            "Epoch 937/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6874 - accuracy: 0.6861 - val_loss: 0.7066 - val_accuracy: 0.7226\n",
            "Epoch 938/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6959 - accuracy: 0.6792 - val_loss: 0.7051 - val_accuracy: 0.7226\n",
            "Epoch 939/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6950 - accuracy: 0.6833 - val_loss: 0.7052 - val_accuracy: 0.7290\n",
            "Epoch 940/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7015 - accuracy: 0.6944 - val_loss: 0.7051 - val_accuracy: 0.7290\n",
            "Epoch 941/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6852 - accuracy: 0.7042 - val_loss: 0.7044 - val_accuracy: 0.7355\n",
            "Epoch 942/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6843 - accuracy: 0.6889 - val_loss: 0.7050 - val_accuracy: 0.7290\n",
            "Epoch 943/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7085 - accuracy: 0.6861 - val_loss: 0.7054 - val_accuracy: 0.7290\n",
            "Epoch 944/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6860 - accuracy: 0.6722 - val_loss: 0.7054 - val_accuracy: 0.7290\n",
            "Epoch 945/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.7000 - val_loss: 0.7047 - val_accuracy: 0.7355\n",
            "Epoch 946/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6943 - accuracy: 0.6833 - val_loss: 0.7042 - val_accuracy: 0.7419\n",
            "Epoch 947/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6880 - accuracy: 0.6833 - val_loss: 0.7046 - val_accuracy: 0.7355\n",
            "Epoch 948/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6804 - accuracy: 0.7083 - val_loss: 0.7038 - val_accuracy: 0.7419\n",
            "Epoch 949/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6871 - accuracy: 0.6917 - val_loss: 0.7045 - val_accuracy: 0.7290\n",
            "Epoch 950/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6978 - accuracy: 0.6792 - val_loss: 0.7043 - val_accuracy: 0.7290\n",
            "Epoch 951/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6828 - accuracy: 0.6806 - val_loss: 0.7049 - val_accuracy: 0.7290\n",
            "Epoch 952/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6887 - accuracy: 0.7069 - val_loss: 0.7044 - val_accuracy: 0.7355\n",
            "Epoch 953/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6857 - accuracy: 0.6819 - val_loss: 0.7038 - val_accuracy: 0.7355\n",
            "Epoch 954/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6875 - accuracy: 0.7056 - val_loss: 0.7046 - val_accuracy: 0.7290\n",
            "Epoch 955/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.6931 - val_loss: 0.7040 - val_accuracy: 0.7226\n",
            "Epoch 956/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6900 - accuracy: 0.6833 - val_loss: 0.7041 - val_accuracy: 0.7290\n",
            "Epoch 957/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.6903 - val_loss: 0.7032 - val_accuracy: 0.7290\n",
            "Epoch 958/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6919 - accuracy: 0.6889 - val_loss: 0.7028 - val_accuracy: 0.7419\n",
            "Epoch 959/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6878 - accuracy: 0.6764 - val_loss: 0.7040 - val_accuracy: 0.7290\n",
            "Epoch 960/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6885 - accuracy: 0.6875 - val_loss: 0.7032 - val_accuracy: 0.7290\n",
            "Epoch 961/1000\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6889 - accuracy: 0.6889 - val_loss: 0.7020 - val_accuracy: 0.7419\n",
            "Epoch 962/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6837 - accuracy: 0.6917 - val_loss: 0.7017 - val_accuracy: 0.7419\n",
            "Epoch 963/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6959 - accuracy: 0.6819 - val_loss: 0.7015 - val_accuracy: 0.7419\n",
            "Epoch 964/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.6861 - val_loss: 0.7016 - val_accuracy: 0.7419\n",
            "Epoch 965/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6963 - accuracy: 0.6708 - val_loss: 0.7017 - val_accuracy: 0.7419\n",
            "Epoch 966/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6907 - accuracy: 0.6847 - val_loss: 0.7024 - val_accuracy: 0.7226\n",
            "Epoch 967/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6939 - accuracy: 0.6889 - val_loss: 0.7016 - val_accuracy: 0.7290\n",
            "Epoch 968/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6879 - accuracy: 0.6694 - val_loss: 0.7021 - val_accuracy: 0.7226\n",
            "Epoch 969/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6983 - accuracy: 0.6847 - val_loss: 0.7022 - val_accuracy: 0.7226\n",
            "Epoch 970/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6943 - accuracy: 0.6847 - val_loss: 0.7017 - val_accuracy: 0.7226\n",
            "Epoch 971/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6915 - accuracy: 0.6861 - val_loss: 0.7012 - val_accuracy: 0.7226\n",
            "Epoch 972/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6868 - accuracy: 0.6958 - val_loss: 0.7010 - val_accuracy: 0.7290\n",
            "Epoch 973/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6784 - accuracy: 0.7042 - val_loss: 0.7008 - val_accuracy: 0.7290\n",
            "Epoch 974/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6821 - accuracy: 0.6958 - val_loss: 0.7009 - val_accuracy: 0.7226\n",
            "Epoch 975/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6998 - accuracy: 0.6931 - val_loss: 0.7002 - val_accuracy: 0.7355\n",
            "Epoch 976/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6958 - accuracy: 0.6875 - val_loss: 0.7014 - val_accuracy: 0.7226\n",
            "Epoch 977/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6824 - accuracy: 0.6958 - val_loss: 0.7003 - val_accuracy: 0.7355\n",
            "Epoch 978/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6852 - accuracy: 0.6806 - val_loss: 0.7007 - val_accuracy: 0.7226\n",
            "Epoch 979/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6935 - accuracy: 0.6861 - val_loss: 0.7016 - val_accuracy: 0.7226\n",
            "Epoch 980/1000\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6861 - accuracy: 0.6917 - val_loss: 0.7008 - val_accuracy: 0.7290\n",
            "Epoch 981/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7027 - accuracy: 0.6889 - val_loss: 0.7002 - val_accuracy: 0.7355\n",
            "Epoch 982/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6861 - accuracy: 0.6958 - val_loss: 0.7015 - val_accuracy: 0.7226\n",
            "Epoch 983/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6867 - accuracy: 0.6917 - val_loss: 0.7004 - val_accuracy: 0.7290\n",
            "Epoch 984/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6878 - accuracy: 0.7028 - val_loss: 0.7015 - val_accuracy: 0.7226\n",
            "Epoch 985/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6786 - accuracy: 0.6958 - val_loss: 0.7006 - val_accuracy: 0.7226\n",
            "Epoch 986/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6916 - accuracy: 0.6889 - val_loss: 0.7003 - val_accuracy: 0.7226\n",
            "Epoch 987/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6803 - accuracy: 0.6889 - val_loss: 0.6996 - val_accuracy: 0.7355\n",
            "Epoch 988/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6848 - accuracy: 0.7056 - val_loss: 0.7000 - val_accuracy: 0.7226\n",
            "Epoch 989/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6890 - accuracy: 0.6806 - val_loss: 0.7004 - val_accuracy: 0.7226\n",
            "Epoch 990/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6756 - accuracy: 0.6833 - val_loss: 0.6993 - val_accuracy: 0.7290\n",
            "Epoch 991/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6845 - accuracy: 0.6986 - val_loss: 0.7003 - val_accuracy: 0.7290\n",
            "Epoch 992/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6891 - accuracy: 0.6653 - val_loss: 0.6988 - val_accuracy: 0.7355\n",
            "Epoch 993/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.6722 - val_loss: 0.6992 - val_accuracy: 0.7355\n",
            "Epoch 994/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.6833 - val_loss: 0.6988 - val_accuracy: 0.7355\n",
            "Epoch 995/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6883 - accuracy: 0.6861 - val_loss: 0.6984 - val_accuracy: 0.7355\n",
            "Epoch 996/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7003 - accuracy: 0.6833 - val_loss: 0.6992 - val_accuracy: 0.7290\n",
            "Epoch 997/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6847 - accuracy: 0.6917 - val_loss: 0.6987 - val_accuracy: 0.7290\n",
            "Epoch 998/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6808 - accuracy: 0.6792 - val_loss: 0.6995 - val_accuracy: 0.7226\n",
            "Epoch 999/1000\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6788 - accuracy: 0.6903 - val_loss: 0.6993 - val_accuracy: 0.7226\n",
            "Epoch 1000/1000\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6859 - accuracy: 0.6708 - val_loss: 0.6985 - val_accuracy: 0.7290\n",
            "Final training loss:  0.6858648657798767\n",
            "Final validation loss:  0.6984537243843079\n",
            "Final training accuracy:  0.6708333492279053\n",
            "Final validation accuracy:  0.7290322780609131\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.6600 - accuracy: 0.7032\n",
            "Tester: Classification model learning OK\n",
            "Tester: All tests were successful.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
